{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10276163,"sourceType":"datasetVersion","datasetId":6358473}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nMIXUP ON NEW VALIDATED DATASET\n\nTRAINED ON 250 SAMPLES PER CLASS\nVALIDATED ON 50 SAMPLES PER CLASS\n\n              precision    recall  f1-score   support\n\n         GNB       0.84      0.84      0.84        50\n         GNC       0.89      0.80      0.84        50\n         GPB       0.81      0.84      0.82        50\n         GPC       0.79      0.84      0.82        50\n\n    accuracy                           0.83       200\n   macro avg       0.83      0.83      0.83       200\nweighted avg       0.83      0.83      0.83       200\n\nClass-wise Sensitivity (Recall): [0.84 0.8  0.84 0.84]\nClass-wise Specificity: [0.94666667 0.96666667 0.93333333 0.92666667]\n\n\n\nTRAINED ON ALL DATA\nVALIDATED ON 50 SAMPLES PER CLASS\n\n              precision    recall  f1-score   support\n\n         GNB       0.85      0.94      0.90        50\n         GNC       0.90      0.76      0.83        50\n         GPB       0.91      0.82      0.86        50\n         GPC       0.76      0.88      0.81        50\n\n    accuracy                           0.85       200\n   macro avg       0.86      0.85      0.85       200\nweighted avg       0.86      0.85      0.85       200\n\nClass-wise Sensitivity (Recall): [0.94 0.76 0.82 0.88]\nClass-wise Specificity: [0.94666667 0.97333333 0.97333333 0.90666667]\n\n\n'''\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:05.570551Z","iopub.execute_input":"2024-12-23T07:25:05.570896Z","iopub.status.idle":"2024-12-23T07:25:05.580373Z","shell.execute_reply.started":"2024-12-23T07:25:05.570865Z","shell.execute_reply":"2024-12-23T07:25:05.579560Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\n\nfrom collections import defaultdict\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport timm\n\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, classification_report, roc_auc_score, recall_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nSEED = 16\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:00:46.181931Z","iopub.execute_input":"2025-01-19T06:00:46.182165Z","iopub.status.idle":"2025-01-19T06:00:52.810084Z","shell.execute_reply.started":"2025-01-19T06:00:46.182139Z","shell.execute_reply":"2025-01-19T06:00:52.809162Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# HELPER FUNCTIONS\n\ndef get_sample_count(folder_path):\n    output = {}\n    classes = os.listdir(folder_path)\n    for i in classes:\n        output[i] = len(os.listdir(f'{folder_path}/{i}'))\n    return output\n\n\ndef mixup_images(input_folder_path, output_dir,image_rescale_size=(300, 300),  n=100, alpha=0.4):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    image_paths = [os.path.join(input_folder_path, f) for f in os.listdir(input_folder_path) if os.path.isfile(os.path.join(input_folder_path, f))]\n    if len(image_paths) < 2:\n        return \"Error: There must be at least two images in the input folder.\"\n\n    transform = transforms.Compose([\n        transforms.Resize(image_rescale_size),  \n        transforms.RandomHorizontalFlip(p=0.5), \n        transforms.RandomVerticalFlip(p=0.5),  \n        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1), \n        transforms.ToTensor()\n    ])\n    to_pil = transforms.ToPILImage()\n    \n    image_pairs = []\n    for i in range(len(image_paths)):\n        for j in range(i+1, len(image_paths)):\n            image_pairs.append([image_paths[i], image_paths[j]])\n   \n    random.shuffle(image_pairs)\n\n    for i in range(n):\n        if len(image_pairs) == 0:\n            break\n\n        img_path1, img_path2 = image_pairs.pop()\n        img1 = transform(Image.open(img_path1).convert(\"RGB\"))\n        img2 = transform(Image.open(img_path2).convert(\"RGB\"))\n        lam = np.random.beta(alpha, alpha)\n        mixed_image = lam * img1 + (1 - lam) * img2\n        mixed_image_pil = to_pil(mixed_image.clamp(0, 1)) \n        output_path = os.path.join(output_dir, f\"mixed_image_{i + 1}.jpg\")\n        mixed_image_pil.save(output_path)\n\n\ndef balance_folder(path, sample_size):\n    all_files = os.listdir(path)\n    if sample_size > len(all_files):\n        print(\"SAMPLE_SIZE TOO BIG\")\n        return\n\n    to_delete = len(all_files) - sample_size\n    print(f\"THERE ARE {len(all_files)} FILES at {path}\")\n    print(f\"{to_delete} FILES WILL BE REMOVED\")\n\n    for i in range(to_delete):\n        index = random.randint(0, len(all_files) - 1)\n        os.remove(path + '/' + all_files[index])\n        all_files.remove(all_files[index])\n\n    print('DONE')\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:00:59.409086Z","iopub.execute_input":"2025-01-19T06:00:59.409768Z","iopub.status.idle":"2025-01-19T06:00:59.420405Z","shell.execute_reply.started":"2025-01-19T06:00:59.409735Z","shell.execute_reply":"2025-01-19T06:00:59.419494Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_path = '/kaggle/input/dataset/Train'\nvalidation_set_path = '/kaggle/input/dataset/Val'\n\nprint(get_sample_count(dataset_path))\nprint(get_sample_count(validation_set_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:00:59.516879Z","iopub.execute_input":"2025-01-19T06:00:59.517125Z","iopub.status.idle":"2025-01-19T06:00:59.575627Z","shell.execute_reply.started":"2025-01-19T06:00:59.517101Z","shell.execute_reply":"2025-01-19T06:00:59.574899Z"}},"outputs":[{"name":"stdout","text":"{'GNC': 200, 'GNB': 1457, 'GPC': 1505, 'GPB': 746}\n{'GNC': 50, 'GNB': 50, 'GPC': 50, 'GPB': 50}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Check for CUDA device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(DEVICE)\n\n# Parameters\nmodel_name = 'tiny_vit_21m_512.dist_in22k_ft_in1k'\nnum_epochs = 20\nnum_classes = 4\nbatch_size = 14\nk_folds = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:11.988148Z","iopub.execute_input":"2024-12-23T07:25:11.988407Z","iopub.status.idle":"2024-12-23T07:25:12.014406Z","shell.execute_reply.started":"2024-12-23T07:25:11.988382Z","shell.execute_reply":"2024-12-23T07:25:12.013510Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"m = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\nmodel_info = m.default_cfg\ndel m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:12.015608Z","iopub.execute_input":"2024-12-23T07:25:12.016009Z","iopub.status.idle":"2024-12-23T07:25:16.099437Z","shell.execute_reply.started":"2024-12-23T07:25:12.015966Z","shell.execute_reply":"2024-12-23T07:25:16.098554Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/85.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd2dd614691478b8a9926df60add667"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"input_shape = model_info['input_size'][1:]\ntransform_mean = model_info['mean']\ntransform_std = model_info['std']\n\nprint(f\"USING MODEL ARCHITECTURE {model_info['architecture']} \")\nprint(f\"INPUT SHAPE = {input_shape}\")\nprint(f\"       MEAN = {transform_mean}\")\nprint(f\"        STD = {transform_std}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:16.100557Z","iopub.execute_input":"2024-12-23T07:25:16.100827Z","iopub.status.idle":"2024-12-23T07:25:16.107145Z","shell.execute_reply.started":"2024-12-23T07:25:16.100802Z","shell.execute_reply":"2024-12-23T07:25:16.106229Z"}},"outputs":[{"name":"stdout","text":"USING MODEL ARCHITECTURE tiny_vit_21m_512 \nINPUT SHAPE = (512, 512)\n       MEAN = (0.485, 0.456, 0.406)\n        STD = (0.229, 0.224, 0.225)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"transform = transforms.Compose([\n        transforms.Resize(input_shape),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=transform_mean, std=transform_std)\n    ])\n\ndataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n\nprint(\"CLASS MAPPING\")\nprint(dataset.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:16.108287Z","iopub.execute_input":"2024-12-23T07:25:16.108577Z","iopub.status.idle":"2024-12-23T07:25:19.128025Z","shell.execute_reply.started":"2024-12-23T07:25:16.108551Z","shell.execute_reply":"2024-12-23T07:25:19.127076Z"}},"outputs":[{"name":"stdout","text":"CLASS MAPPING\n{'GNB': 0, 'GNC': 1, 'GPB': 2, 'GPC': 3}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n        return F_loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:19.129272Z","iopub.execute_input":"2024-12-23T07:25:19.129680Z","iopub.status.idle":"2024-12-23T07:25:19.135713Z","shell.execute_reply.started":"2024-12-23T07:25:19.129636Z","shell.execute_reply":"2024-12-23T07:25:19.134677Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n\nhistory = []\n\n# K-Fold Loop\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset, dataset.targets)):\n    print('\\n--------------------------------')\n    print(f'FOLD {fold+1}/{k_folds}')\n    print('--------------------------------')\n\n    # Create data loaders for this fold\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n\n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler, num_workers=4)\n    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler, num_workers=4)\n\n    # Initialize the model for this fold\n    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\n    model = model.to(DEVICE)\n\n    criterion = FocalLoss()\n    # criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    # Variables to keep track of the best model for this fold\n    best_val_acc = -999\n    best_epoch = 0\n    best_model_wts = None\n    best_preds = {\n        'labels':None,\n        'preds':None\n    }\n\n    # Model training loop START\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        model.train()\n        running_loss = 0.0\n\n        # Training\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        all_labels = []\n        all_preds = []\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n\n        val_acc = correct / total\n\n        print(f'Loss: {running_loss/len(train_loader)} , Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {val_acc}')\n\n        # Always keep track of the best model and best output\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch + 1\n            best_model_wts = model.state_dict()\n            best_preds = {\n                'labels': all_labels,\n                'preds': all_preds\n            }\n\n\n    # TRAIN LOOP ENDS HERE\n    all_labels = best_preds['labels']\n    all_preds = best_preds['preds']\n    \n    # Calculate confusion matrix\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    \n    # Sensitivity, Specificity calculation\n    sensitivity = []\n    specificity = []\n    \n    for i in range(num_classes):\n        tp = conf_matrix[i, i]\n        fn = sum(conf_matrix[i, :]) - tp\n        fp = sum(conf_matrix[:, i]) - tp\n        tn = conf_matrix.sum() - (tp + fp + fn)\n\n        sensitivity.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='macro')\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    recall = recall_score(all_labels, all_preds, average='macro')\n\n    best_hist = { 'Accuracy': accuracy, 'Precision': precision, 'F1 Score': f1, 'recall': recall }\n\n    for class_name in dataset.class_to_idx:\n        best_hist[f'{class_name} Sensitivity'] = sensitivity[dataset.class_to_idx[class_name]]\n        best_hist[f'{class_name} Specificity'] = specificity[dataset.class_to_idx[class_name]]\n\n    print()\n    for key, value in best_hist.items():\n        print(f'{key}: {value}')\n\n    print()\n    # get classification report\n    print(classification_report(all_labels, all_preds, target_names=[i for i in dataset.class_to_idx]))\n    print()\n\n    # Save model to disk\n    print(f'Best Epoch for Fold {fold+1}: {best_epoch} with Validation Accuracy: {best_val_acc}')\n    torch.save(best_model_wts, f'fold{fold+1}_model.pth')\n    \n    history.append(best_hist)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T07:25:19.137107Z","iopub.execute_input":"2024-12-23T07:25:19.137385Z","iopub.status.idle":"2024-12-23T13:45:06.805054Z","shell.execute_reply.started":"2024-12-23T07:25:19.137360Z","shell.execute_reply":"2024-12-23T13:45:06.803912Z"}},"outputs":[{"name":"stdout","text":"\n--------------------------------\nFOLD 1/5\n--------------------------------\nEpoch 1/20\nLoss: 0.11717063640077997 , Val Loss: 0.07638190350761372, Val Accuracy: 0.7749360613810742\nEpoch 2/20\nLoss: 0.0648724103895282 , Val Loss: 0.05630394573589521, Val Accuracy: 0.8248081841432225\nEpoch 3/20\nLoss: 0.04864463819415375 , Val Loss: 0.04166151732871575, Val Accuracy: 0.870843989769821\nEpoch 4/20\nLoss: 0.042635135417055735 , Val Loss: 0.04273778775573841, Val Accuracy: 0.860613810741688\nEpoch 5/20\nLoss: 0.03641462211505443 , Val Loss: 0.03734082887448104, Val Accuracy: 0.8772378516624041\nEpoch 6/20\nLoss: 0.030930963604727628 , Val Loss: 0.0323444791221326, Val Accuracy: 0.8797953964194374\nEpoch 7/20\nLoss: 0.030190895130674886 , Val Loss: 0.034476560598704964, Val Accuracy: 0.8938618925831202\nEpoch 8/20\nLoss: 0.026672805324778892 , Val Loss: 0.03437107445123339, Val Accuracy: 0.8849104859335039\nEpoch 9/20\nLoss: 0.02295732185302768 , Val Loss: 0.036068123806866685, Val Accuracy: 0.8887468030690537\nEpoch 10/20\nLoss: 0.02222575136537281 , Val Loss: 0.033301858813501894, Val Accuracy: 0.8951406649616368\nEpoch 11/20\nLoss: 0.019455547185186464 , Val Loss: 0.03117784267150065, Val Accuracy: 0.8913043478260869\nEpoch 12/20\nLoss: 0.017025546444139245 , Val Loss: 0.03291878442230102, Val Accuracy: 0.8976982097186701\nEpoch 13/20\nLoss: 0.016189681737159844 , Val Loss: 0.030474824184368896, Val Accuracy: 0.8746803069053708\nEpoch 14/20\nLoss: 0.015939383058788996 , Val Loss: 0.03298430745365165, Val Accuracy: 0.8836317135549873\nEpoch 15/20\nLoss: 0.016750409955420764 , Val Loss: 0.037442508188438035, Val Accuracy: 0.8721227621483376\nEpoch 16/20\nLoss: 0.013778647788318008 , Val Loss: 0.03262700409894543, Val Accuracy: 0.8964194373401535\nEpoch 17/20\nLoss: 0.013426107892168406 , Val Loss: 0.036414138047673204, Val Accuracy: 0.9002557544757033\nEpoch 18/20\nLoss: 0.011955176654607515 , Val Loss: 0.0340274021946243, Val Accuracy: 0.8989769820971867\nEpoch 19/20\nLoss: 0.012443099486258038 , Val Loss: 0.03261314134579152, Val Accuracy: 0.8900255754475703\nEpoch 20/20\nLoss: 0.010471993132146704 , Val Loss: 0.034951149931917565, Val Accuracy: 0.8938618925831202\n\nAccuracy: 0.9002557544757033\nPrecision: 0.8717516402810689\nF1 Score: 0.8460631687258553\nrecall: 0.8270411986706077\nGNB Sensitivity: 0.9623287671232876\nGNB Specificity: 0.936734693877551\nGNC Sensitivity: 0.6\nGNC Specificity: 0.9905660377358491\nGPB Sensitivity: 0.8322147651006712\nGPB Specificity: 0.9778830963665087\nGPC Sensitivity: 0.9136212624584718\nGPC Specificity: 0.9459459459459459\n\n              precision    recall  f1-score   support\n\n         GNB       0.90      0.96      0.93       292\n         GNC       0.77      0.60      0.68        40\n         GPB       0.90      0.83      0.86       149\n         GPC       0.91      0.91      0.91       301\n\n    accuracy                           0.90       782\n   macro avg       0.87      0.83      0.85       782\nweighted avg       0.90      0.90      0.90       782\n\n\nBest Epoch for Fold 1: 17 with Validation Accuracy: 0.9002557544757033\n\n--------------------------------\nFOLD 2/5\n--------------------------------\nEpoch 1/20\nLoss: 0.11997581163554319 , Val Loss: 0.07582563447899052, Val Accuracy: 0.7902813299232737\nEpoch 2/20\nLoss: 0.0674500027837764 , Val Loss: 0.0482269964580025, Val Accuracy: 0.8593350383631714\nEpoch 3/20\nLoss: 0.05035003667165126 , Val Loss: 0.0459204368526116, Val Accuracy: 0.8567774936061381\nEpoch 4/20\nLoss: 0.04213507998169267 , Val Loss: 0.04239295442987766, Val Accuracy: 0.8631713554987213\nEpoch 5/20\nLoss: 0.037433240425473614 , Val Loss: 0.03924793283554858, Val Accuracy: 0.8580562659846548\nEpoch 6/20\nLoss: 0.035220163267305385 , Val Loss: 0.037807975558929945, Val Accuracy: 0.8734015345268542\nEpoch 7/20\nLoss: 0.028850255103731928 , Val Loss: 0.04756508518974962, Val Accuracy: 0.8388746803069054\nEpoch 8/20\nLoss: 0.028542514160757752 , Val Loss: 0.044008306826331785, Val Accuracy: 0.8542199488491049\nEpoch 9/20\nLoss: 0.024325976970950642 , Val Loss: 0.0367767502903007, Val Accuracy: 0.8836317135549873\nEpoch 10/20\nLoss: 0.022565740213654602 , Val Loss: 0.031729933046985286, Val Accuracy: 0.8976982097186701\nEpoch 11/20\nLoss: 0.01952980136307555 , Val Loss: 0.03686758315390242, Val Accuracy: 0.8887468030690537\nEpoch 12/20\nLoss: 0.01917563841613758 , Val Loss: 0.03739595622547703, Val Accuracy: 0.8887468030690537\nEpoch 13/20\nLoss: 0.01601507220532637 , Val Loss: 0.034465920035602594, Val Accuracy: 0.887468030690537\nEpoch 14/20\nLoss: 0.017957122943438923 , Val Loss: 0.036050195809886124, Val Accuracy: 0.8836317135549873\nEpoch 15/20\nLoss: 0.015502905017326287 , Val Loss: 0.037327049249142874, Val Accuracy: 0.8849104859335039\nEpoch 16/20\nLoss: 0.01399763613205453 , Val Loss: 0.03502020663706519, Val Accuracy: 0.8938618925831202\nEpoch 17/20\nLoss: 0.013439423049151498 , Val Loss: 0.034328631159691474, Val Accuracy: 0.90153452685422\nEpoch 18/20\nLoss: 0.012101383416718039 , Val Loss: 0.03568429592581067, Val Accuracy: 0.8925831202046036\nEpoch 19/20\nLoss: 0.011286207321323414 , Val Loss: 0.04401340960508345, Val Accuracy: 0.8887468030690537\nEpoch 20/20\nLoss: 0.00983571282761529 , Val Loss: 0.04320860409227732, Val Accuracy: 0.870843989769821\n\nAccuracy: 0.90153452685422\nPrecision: 0.8466063423543084\nF1 Score: 0.8264686446264207\nrecall: 0.8126651279162926\nGNB Sensitivity: 0.9212328767123288\nGNB Specificity: 0.9673469387755103\nGNC Sensitivity: 0.5\nGNC Specificity: 0.9865229110512129\nGPB Sensitivity: 0.8859060402684564\nGPB Specificity: 0.9715639810426541\nGPC Sensitivity: 0.9435215946843853\nGPC Specificity: 0.9313929313929314\n\n              precision    recall  f1-score   support\n\n         GNB       0.94      0.92      0.93       292\n         GNC       0.67      0.50      0.57        40\n         GPB       0.88      0.89      0.88       149\n         GPC       0.90      0.94      0.92       301\n\n    accuracy                           0.90       782\n   macro avg       0.85      0.81      0.83       782\nweighted avg       0.90      0.90      0.90       782\n\n\nBest Epoch for Fold 2: 17 with Validation Accuracy: 0.90153452685422\n\n--------------------------------\nFOLD 3/5\n--------------------------------\nEpoch 1/20\nLoss: 0.12519037595484406 , Val Loss: 0.0765516575026725, Val Accuracy: 0.7864450127877238\nEpoch 2/20\nLoss: 0.0682536912874119 , Val Loss: 0.05631197363670383, Val Accuracy: 0.8260869565217391\nEpoch 3/20\nLoss: 0.054658192594485754 , Val Loss: 0.042129735628675134, Val Accuracy: 0.8721227621483376\nEpoch 4/20\nLoss: 0.04520777370115476 , Val Loss: 0.04514426184219441, Val Accuracy: 0.8478260869565217\nEpoch 5/20\nLoss: 0.0376880827201863 , Val Loss: 0.03696917183697224, Val Accuracy: 0.8849104859335039\nEpoch 6/20\nLoss: 0.033285504698142176 , Val Loss: 0.037028784481143315, Val Accuracy: 0.8836317135549873\nEpoch 7/20\nLoss: 0.02829563258793704 , Val Loss: 0.02560774764528365, Val Accuracy: 0.9207161125319693\nEpoch 8/20\nLoss: 0.026069380109090292 , Val Loss: 0.03259614065505697, Val Accuracy: 0.8951406649616368\nEpoch 9/20\nLoss: 0.023302958074574626 , Val Loss: 0.030414507146425813, Val Accuracy: 0.8976982097186701\nEpoch 10/20\nLoss: 0.02136519497019305 , Val Loss: 0.03168754598716207, Val Accuracy: 0.8989769820971867\nEpoch 11/20\nLoss: 0.01987427060547426 , Val Loss: 0.030245834728702903, Val Accuracy: 0.90153452685422\nEpoch 12/20\nLoss: 0.017827042447087087 , Val Loss: 0.029807301542730005, Val Accuracy: 0.90153452685422\nEpoch 13/20\nLoss: 0.01754754433308595 , Val Loss: 0.040919064972383366, Val Accuracy: 0.8631713554987213\nEpoch 14/20\nLoss: 0.016069509074571085 , Val Loss: 0.03170350067791462, Val Accuracy: 0.9053708439897699\nEpoch 15/20\nLoss: 0.014618669798437622 , Val Loss: 0.032367325209114436, Val Accuracy: 0.8976982097186701\nEpoch 16/20\nLoss: 0.014923014626999378 , Val Loss: 0.03480419197252819, Val Accuracy: 0.8989769820971867\nEpoch 17/20\nLoss: 0.011296484703052556 , Val Loss: 0.03213224000968954, Val Accuracy: 0.9117647058823529\nEpoch 18/20\nLoss: 0.011131849905853284 , Val Loss: 0.033546039097668005, Val Accuracy: 0.9028132992327366\nEpoch 19/20\nLoss: 0.012985384290378274 , Val Loss: 0.03130760546628153, Val Accuracy: 0.9117647058823529\nEpoch 20/20\nLoss: 0.010258839829215763 , Val Loss: 0.03573540538335302, Val Accuracy: 0.9002557544757033\n\nAccuracy: 0.9207161125319693\nPrecision: 0.8840511943621795\nF1 Score: 0.8697587393039703\nrecall: 0.8589413010469111\nGNB Sensitivity: 0.9621993127147767\nGNB Specificity: 0.9775967413441955\nGNC Sensitivity: 0.65\nGNC Specificity: 0.9905660377358491\nGPB Sensitivity: 0.8933333333333333\nGPB Specificity: 0.9651898734177216\nGPC Sensitivity: 0.9302325581395349\nGPC Specificity: 0.9542619542619543\n\n              precision    recall  f1-score   support\n\n         GNB       0.96      0.96      0.96       291\n         GNC       0.79      0.65      0.71        40\n         GPB       0.86      0.89      0.88       150\n         GPC       0.93      0.93      0.93       301\n\n    accuracy                           0.92       782\n   macro avg       0.88      0.86      0.87       782\nweighted avg       0.92      0.92      0.92       782\n\n\nBest Epoch for Fold 3: 7 with Validation Accuracy: 0.9207161125319693\n\n--------------------------------\nFOLD 4/5\n--------------------------------\nEpoch 1/20\nLoss: 0.11558668335367527 , Val Loss: 0.07432486140169203, Val Accuracy: 0.8002560819462228\nEpoch 2/20\nLoss: 0.06750287709707793 , Val Loss: 0.04968504822214267, Val Accuracy: 0.8425096030729834\nEpoch 3/20\nLoss: 0.05196685585542582 , Val Loss: 0.04729644900986126, Val Accuracy: 0.8335467349551856\nEpoch 4/20\nLoss: 0.04595365608111024 , Val Loss: 0.0378372839020033, Val Accuracy: 0.8745198463508322\nEpoch 5/20\nLoss: 0.039849427137856504 , Val Loss: 0.036692621818344505, Val Accuracy: 0.8617157490396927\nEpoch 6/20\nLoss: 0.03800813841059737 , Val Loss: 0.03149462778986033, Val Accuracy: 0.8924455825864277\nEpoch 7/20\nLoss: 0.030134562869664348 , Val Loss: 0.03152702458568716, Val Accuracy: 0.8886043533930857\nEpoch 8/20\nLoss: 0.02957853252363358 , Val Loss: 0.03379537083674222, Val Accuracy: 0.8796414852752881\nEpoch 9/20\nLoss: 0.026580849898372044 , Val Loss: 0.027806806032978266, Val Accuracy: 0.910371318822023\nEpoch 10/20\nLoss: 0.02471309892176318 , Val Loss: 0.03097051414494802, Val Accuracy: 0.8809218950064021\nEpoch 11/20\nLoss: 0.02116701473226255 , Val Loss: 0.031865626124532094, Val Accuracy: 0.8937259923175416\nEpoch 12/20\nLoss: 0.019343590522891776 , Val Loss: 0.034665698218824606, Val Accuracy: 0.8873239436619719\nEpoch 13/20\nLoss: 0.017046229665408776 , Val Loss: 0.03234633397992833, Val Accuracy: 0.8924455825864277\nEpoch 14/20\nLoss: 0.017188493008559038 , Val Loss: 0.028577618156435034, Val Accuracy: 0.9039692701664532\nEpoch 15/20\nLoss: 0.015833079960170186 , Val Loss: 0.03194809737864749, Val Accuracy: 0.8988476312419974\nEpoch 16/20\nLoss: 0.014664439323366554 , Val Loss: 0.030642522998927495, Val Accuracy: 0.8975672215108835\nEpoch 17/20\nLoss: 0.012104929052449214 , Val Loss: 0.030724509694014808, Val Accuracy: 0.8937259923175416\nEpoch 18/20\nLoss: 0.013220038146625614 , Val Loss: 0.031646513587994765, Val Accuracy: 0.9078104993597952\nEpoch 19/20\nLoss: 0.012650099341174479 , Val Loss: 0.03589041974295729, Val Accuracy: 0.8847631241997439\nEpoch 20/20\nLoss: 0.010571500096765314 , Val Loss: 0.028940909728199977, Val Accuracy: 0.8937259923175416\n\nAccuracy: 0.910371318822023\nPrecision: 0.8731506830374427\nF1 Score: 0.8685539999221568\nrecall: 0.8662602073134449\nGNB Sensitivity: 0.9518900343642611\nGNB Specificity: 0.9653061224489796\nGNC Sensitivity: 0.7\nGNC Specificity: 0.9892037786774629\nGPB Sensitivity: 0.9194630872483222\nGPB Specificity: 0.9572784810126582\nGPC Sensitivity: 0.893687707641196\nGPC Specificity: 0.9625\n\n              precision    recall  f1-score   support\n\n         GNB       0.94      0.95      0.95       291\n         GNC       0.78      0.70      0.74        40\n         GPB       0.84      0.92      0.88       149\n         GPC       0.94      0.89      0.91       301\n\n    accuracy                           0.91       781\n   macro avg       0.87      0.87      0.87       781\nweighted avg       0.91      0.91      0.91       781\n\n\nBest Epoch for Fold 4: 9 with Validation Accuracy: 0.910371318822023\n\n--------------------------------\nFOLD 5/5\n--------------------------------\nEpoch 1/20\nLoss: 0.12349738391848016 , Val Loss: 0.09268682596406766, Val Accuracy: 0.7323943661971831\nEpoch 2/20\nLoss: 0.06732272623672284 , Val Loss: 0.05404758579762919, Val Accuracy: 0.8348271446862996\nEpoch 3/20\nLoss: 0.05220158363226801 , Val Loss: 0.03686291476645108, Val Accuracy: 0.8847631241997439\nEpoch 4/20\nLoss: 0.04315267548788272 , Val Loss: 0.03670622903155163, Val Accuracy: 0.8758002560819462\nEpoch 5/20\nLoss: 0.03712713751675827 , Val Loss: 0.033165571986631094, Val Accuracy: 0.8860435339308579\nEpoch 6/20\nLoss: 0.032437652336998975 , Val Loss: 0.03125561380459528, Val Accuracy: 0.8886043533930857\nEpoch 7/20\nLoss: 0.029425922350908098 , Val Loss: 0.03325248039410716, Val Accuracy: 0.8745198463508322\nEpoch 8/20\nLoss: 0.027108918523091625 , Val Loss: 0.032974227697455456, Val Accuracy: 0.8886043533930857\nEpoch 9/20\nLoss: 0.025769341125851497 , Val Loss: 0.033856422711063976, Val Accuracy: 0.8886043533930857\nEpoch 10/20\nLoss: 0.02352869646243302 , Val Loss: 0.030530295991671404, Val Accuracy: 0.9052496798975672\nEpoch 11/20\nLoss: 0.01991188953044392 , Val Loss: 0.03392676475258278, Val Accuracy: 0.8834827144686299\nEpoch 12/20\nLoss: 0.017589758258379464 , Val Loss: 0.03377891568899421, Val Accuracy: 0.8886043533930857\nEpoch 13/20\nLoss: 0.016957513937606045 , Val Loss: 0.03435155919370508, Val Accuracy: 0.8770806658130602\nEpoch 14/20\nLoss: 0.014045940019579055 , Val Loss: 0.03272052308602724, Val Accuracy: 0.9001280409731114\nEpoch 15/20\nLoss: 0.01435826518068747 , Val Loss: 0.035778043519323025, Val Accuracy: 0.8758002560819462\nEpoch 16/20\nLoss: 0.014275395400545676 , Val Loss: 0.0306643349343046, Val Accuracy: 0.9052496798975672\nEpoch 17/20\nLoss: 0.012040769658856984 , Val Loss: 0.03710673000646888, Val Accuracy: 0.8809218950064021\nEpoch 18/20\nLoss: 0.012894220799872918 , Val Loss: 0.035883748487062155, Val Accuracy: 0.8911651728553137\nEpoch 19/20\nLoss: 0.011732987050891097 , Val Loss: 0.032620761101238065, Val Accuracy: 0.8975672215108835\nEpoch 20/20\nLoss: 0.010440932112162824 , Val Loss: 0.03216479504875939, Val Accuracy: 0.8937259923175416\n\nAccuracy: 0.9052496798975672\nPrecision: 0.8905320714144244\nF1 Score: 0.8698546579582795\nrecall: 0.8530776654791001\nGNB Sensitivity: 0.9484536082474226\nGNB Specificity: 0.9571428571428572\nGNC Sensitivity: 0.725\nGNC Specificity: 0.9932523616734144\nGPB Sensitivity: 0.7986577181208053\nGPB Specificity: 0.9746835443037974\nGPC Sensitivity: 0.9401993355481728\nGPC Specificity: 0.9333333333333333\n\n              precision    recall  f1-score   support\n\n         GNB       0.93      0.95      0.94       291\n         GNC       0.85      0.72      0.78        40\n         GPB       0.88      0.80      0.84       149\n         GPC       0.90      0.94      0.92       301\n\n    accuracy                           0.91       781\n   macro avg       0.89      0.85      0.87       781\nweighted avg       0.90      0.91      0.90       781\n\n\nBest Epoch for Fold 5: 10 with Validation Accuracy: 0.9052496798975672\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T13:45:06.806906Z","iopub.execute_input":"2024-12-23T13:45:06.807809Z","iopub.status.idle":"2024-12-23T13:45:06.814812Z","shell.execute_reply.started":"2024-12-23T13:45:06.807765Z","shell.execute_reply":"2024-12-23T13:45:06.814022Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[{'Accuracy': 0.9002557544757033,\n  'Precision': 0.8717516402810689,\n  'F1 Score': 0.8460631687258553,\n  'recall': 0.8270411986706077,\n  'GNB Sensitivity': 0.9623287671232876,\n  'GNB Specificity': 0.936734693877551,\n  'GNC Sensitivity': 0.6,\n  'GNC Specificity': 0.9905660377358491,\n  'GPB Sensitivity': 0.8322147651006712,\n  'GPB Specificity': 0.9778830963665087,\n  'GPC Sensitivity': 0.9136212624584718,\n  'GPC Specificity': 0.9459459459459459},\n {'Accuracy': 0.90153452685422,\n  'Precision': 0.8466063423543084,\n  'F1 Score': 0.8264686446264207,\n  'recall': 0.8126651279162926,\n  'GNB Sensitivity': 0.9212328767123288,\n  'GNB Specificity': 0.9673469387755103,\n  'GNC Sensitivity': 0.5,\n  'GNC Specificity': 0.9865229110512129,\n  'GPB Sensitivity': 0.8859060402684564,\n  'GPB Specificity': 0.9715639810426541,\n  'GPC Sensitivity': 0.9435215946843853,\n  'GPC Specificity': 0.9313929313929314},\n {'Accuracy': 0.9207161125319693,\n  'Precision': 0.8840511943621795,\n  'F1 Score': 0.8697587393039703,\n  'recall': 0.8589413010469111,\n  'GNB Sensitivity': 0.9621993127147767,\n  'GNB Specificity': 0.9775967413441955,\n  'GNC Sensitivity': 0.65,\n  'GNC Specificity': 0.9905660377358491,\n  'GPB Sensitivity': 0.8933333333333333,\n  'GPB Specificity': 0.9651898734177216,\n  'GPC Sensitivity': 0.9302325581395349,\n  'GPC Specificity': 0.9542619542619543},\n {'Accuracy': 0.910371318822023,\n  'Precision': 0.8731506830374427,\n  'F1 Score': 0.8685539999221568,\n  'recall': 0.8662602073134449,\n  'GNB Sensitivity': 0.9518900343642611,\n  'GNB Specificity': 0.9653061224489796,\n  'GNC Sensitivity': 0.7,\n  'GNC Specificity': 0.9892037786774629,\n  'GPB Sensitivity': 0.9194630872483222,\n  'GPB Specificity': 0.9572784810126582,\n  'GPC Sensitivity': 0.893687707641196,\n  'GPC Specificity': 0.9625},\n {'Accuracy': 0.9052496798975672,\n  'Precision': 0.8905320714144244,\n  'F1 Score': 0.8698546579582795,\n  'recall': 0.8530776654791001,\n  'GNB Sensitivity': 0.9484536082474226,\n  'GNB Specificity': 0.9571428571428572,\n  'GNC Sensitivity': 0.725,\n  'GNC Specificity': 0.9932523616734144,\n  'GPB Sensitivity': 0.7986577181208053,\n  'GPB Specificity': 0.9746835443037974,\n  'GPC Sensitivity': 0.9401993355481728,\n  'GPC Specificity': 0.9333333333333333}]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"hist_df = pd.DataFrame(history)\n\n# save hist_df to disk\nhist_df.to_csv('KFOLD_CV.csv', index=False)\n\nhist_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T13:45:06.815955Z","iopub.execute_input":"2024-12-23T13:45:06.816262Z","iopub.status.idle":"2024-12-23T13:45:06.851527Z","shell.execute_reply.started":"2024-12-23T13:45:06.816237Z","shell.execute_reply":"2024-12-23T13:45:06.850727Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   Accuracy  Precision  F1 Score    recall  GNB Sensitivity  GNB Specificity  \\\n0  0.900256   0.871752  0.846063  0.827041         0.962329         0.936735   \n1  0.901535   0.846606  0.826469  0.812665         0.921233         0.967347   \n2  0.920716   0.884051  0.869759  0.858941         0.962199         0.977597   \n3  0.910371   0.873151  0.868554  0.866260         0.951890         0.965306   \n4  0.905250   0.890532  0.869855  0.853078         0.948454         0.957143   \n\n   GNC Sensitivity  GNC Specificity  GPB Sensitivity  GPB Specificity  \\\n0            0.600         0.990566         0.832215         0.977883   \n1            0.500         0.986523         0.885906         0.971564   \n2            0.650         0.990566         0.893333         0.965190   \n3            0.700         0.989204         0.919463         0.957278   \n4            0.725         0.993252         0.798658         0.974684   \n\n   GPC Sensitivity  GPC Specificity  \n0         0.913621         0.945946  \n1         0.943522         0.931393  \n2         0.930233         0.954262  \n3         0.893688         0.962500  \n4         0.940199         0.933333  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>F1 Score</th>\n      <th>recall</th>\n      <th>GNB Sensitivity</th>\n      <th>GNB Specificity</th>\n      <th>GNC Sensitivity</th>\n      <th>GNC Specificity</th>\n      <th>GPB Sensitivity</th>\n      <th>GPB Specificity</th>\n      <th>GPC Sensitivity</th>\n      <th>GPC Specificity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.900256</td>\n      <td>0.871752</td>\n      <td>0.846063</td>\n      <td>0.827041</td>\n      <td>0.962329</td>\n      <td>0.936735</td>\n      <td>0.600</td>\n      <td>0.990566</td>\n      <td>0.832215</td>\n      <td>0.977883</td>\n      <td>0.913621</td>\n      <td>0.945946</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.901535</td>\n      <td>0.846606</td>\n      <td>0.826469</td>\n      <td>0.812665</td>\n      <td>0.921233</td>\n      <td>0.967347</td>\n      <td>0.500</td>\n      <td>0.986523</td>\n      <td>0.885906</td>\n      <td>0.971564</td>\n      <td>0.943522</td>\n      <td>0.931393</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.920716</td>\n      <td>0.884051</td>\n      <td>0.869759</td>\n      <td>0.858941</td>\n      <td>0.962199</td>\n      <td>0.977597</td>\n      <td>0.650</td>\n      <td>0.990566</td>\n      <td>0.893333</td>\n      <td>0.965190</td>\n      <td>0.930233</td>\n      <td>0.954262</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.910371</td>\n      <td>0.873151</td>\n      <td>0.868554</td>\n      <td>0.866260</td>\n      <td>0.951890</td>\n      <td>0.965306</td>\n      <td>0.700</td>\n      <td>0.989204</td>\n      <td>0.919463</td>\n      <td>0.957278</td>\n      <td>0.893688</td>\n      <td>0.962500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.905250</td>\n      <td>0.890532</td>\n      <td>0.869855</td>\n      <td>0.853078</td>\n      <td>0.948454</td>\n      <td>0.957143</td>\n      <td>0.725</td>\n      <td>0.993252</td>\n      <td>0.798658</td>\n      <td>0.974684</td>\n      <td>0.940199</td>\n      <td>0.933333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Test Models on validation data\nmodel_dir = './'\n\nval_transform = transforms.Compose([\n        transforms.Resize(input_shape),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=transform_mean, std=transform_std)\n    ])\n\nval_dataset = datasets.ImageFolder(root=validation_set_path, transform=val_transform)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\nprint(\"CLASS MAPPING\")\nprint(val_dataset.class_to_idx)\n\n\nmajority_vote_preds = defaultdict(list)\n\n# Iterate over each saved model for validation\nfor fold in range(1, k_folds + 1):\n    print(f\"Generating Predictions on model {fold}\")\n    # Load the saved model weights\n    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n    model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n    model = model.to(DEVICE)\n    model.eval()\n\n    # Variables to keep track of performance metrics\n    correct = 0\n    total = 0\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(val_loader):\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(inputs)\n            \n            _, predicted = torch.max(outputs, 1)\n\n            # Store predictions for each batch item\n            for idx, prediction in enumerate(predicted.cpu().numpy()):\n                sample_index = i * batch_size + idx\n                majority_vote_preds[sample_index].append(prediction)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = correct / total\n    print(f'Validation Accuracy for Fold {fold}: {accuracy}')\n\n    # Calculate and display other metrics\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n\n    print(f'Fold {fold} Validation Metrics:')\n\n    print(classification_report(all_labels, all_preds, target_names=[i for i in val_dataset.class_to_idx]))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T13:45:06.852633Z","iopub.execute_input":"2024-12-23T13:45:06.852956Z","iopub.status.idle":"2024-12-23T13:45:53.365215Z","shell.execute_reply.started":"2024-12-23T13:45:06.852920Z","shell.execute_reply":"2024-12-23T13:45:53.364048Z"}},"outputs":[{"name":"stdout","text":"CLASS MAPPING\n{'GNB': 0, 'GNC': 1, 'GPB': 2, 'GPC': 3}\nGenerating Predictions on model 1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/627966965.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for Fold 1: 0.87\nFold 1 Validation Metrics:\n              precision    recall  f1-score   support\n\n         GNB       0.89      0.94      0.91        50\n         GNC       0.89      0.82      0.85        50\n         GPB       0.95      0.84      0.89        50\n         GPC       0.77      0.88      0.82        50\n\n    accuracy                           0.87       200\n   macro avg       0.88      0.87      0.87       200\nweighted avg       0.88      0.87      0.87       200\n\n\nGenerating Predictions on model 2\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/627966965.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for Fold 2: 0.85\nFold 2 Validation Metrics:\n              precision    recall  f1-score   support\n\n         GNB       0.87      0.96      0.91        50\n         GNC       0.92      0.66      0.77        50\n         GPB       0.93      0.86      0.90        50\n         GPC       0.73      0.92      0.81        50\n\n    accuracy                           0.85       200\n   macro avg       0.86      0.85      0.85       200\nweighted avg       0.86      0.85      0.85       200\n\n\nGenerating Predictions on model 3\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/627966965.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for Fold 3: 0.84\nFold 3 Validation Metrics:\n              precision    recall  f1-score   support\n\n         GNB       0.83      0.96      0.89        50\n         GNC       0.86      0.74      0.80        50\n         GPB       0.95      0.80      0.87        50\n         GPC       0.75      0.86      0.80        50\n\n    accuracy                           0.84       200\n   macro avg       0.85      0.84      0.84       200\nweighted avg       0.85      0.84      0.84       200\n\n\nGenerating Predictions on model 4\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/627966965.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for Fold 4: 0.84\nFold 4 Validation Metrics:\n              precision    recall  f1-score   support\n\n         GNB       0.84      0.94      0.89        50\n         GNC       0.92      0.66      0.77        50\n         GPB       0.91      0.84      0.87        50\n         GPC       0.74      0.92      0.82        50\n\n    accuracy                           0.84       200\n   macro avg       0.85      0.84      0.84       200\nweighted avg       0.85      0.84      0.84       200\n\n\nGenerating Predictions on model 5\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/627966965.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_dir}/fold{fold}_model.pth', map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy for Fold 5: 0.87\nFold 5 Validation Metrics:\n              precision    recall  f1-score   support\n\n         GNB       0.91      0.96      0.93        50\n         GNC       0.93      0.80      0.86        50\n         GPB       0.91      0.84      0.87        50\n         GPC       0.76      0.88      0.81        50\n\n    accuracy                           0.87       200\n   macro avg       0.88      0.87      0.87       200\nweighted avg       0.88      0.87      0.87       200\n\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Perform majority voting\nfinal_preds = []\nfor sample_index in sorted(majority_vote_preds.keys()):\n    # Majority voting across folds\n    votes = majority_vote_preds[sample_index]\n    final_pred = np.bincount(votes).argmax()  # Get the class with maximum votes\n    final_preds.append(final_pred)\n\n# Collect true labels for evaluation\ntrue_labels = []\nfor inputs, labels in val_loader:\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics based on final predictions\nprint('Final Model Validation Metrics:')\nconf_matrix = confusion_matrix(true_labels, final_preds)\nprint(conf_matrix)\nprint(classification_report(true_labels, final_preds, target_names=[i for i in val_dataset.class_to_idx]))\n\n# Initialize arrays to store sensitivity and specificity for each class\nnum_classes = len(conf_matrix)\nsensitivity = np.zeros(num_classes)\nspecificity = np.zeros(num_classes)\n\n# Calculate sensitivity and specificity for each class\nfor i in range(num_classes):\n    tp = conf_matrix[i, i]  # True Positives\n    fn = np.sum(conf_matrix[i, :]) - tp  # False Negatives\n    fp = np.sum(conf_matrix[:, i]) - tp  # False Positives\n    tn = np.sum(conf_matrix) - (tp + fn + fp)  # True Negatives\n    \n    # Calculate sensitivity and specificity\n    sensitivity[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n    specificity[i] = tn / (tn + fp) if (tn + fp) > 0 else 0\n\n# Display the results\nprint(\"Class-wise Sensitivity (Recall):\", sensitivity)\nprint(\"Class-wise Specificity:\", specificity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T13:45:53.368475Z","iopub.execute_input":"2024-12-23T13:45:53.368770Z","iopub.status.idle":"2024-12-23T13:45:58.388086Z","shell.execute_reply.started":"2024-12-23T13:45:53.368741Z","shell.execute_reply":"2024-12-23T13:45:58.387092Z"}},"outputs":[{"name":"stdout","text":"Final Model Validation Metrics:\n[[47  2  1  0]\n [ 6 38  0  6]\n [ 1  0 41  8]\n [ 1  2  3 44]]\n              precision    recall  f1-score   support\n\n         GNB       0.85      0.94      0.90        50\n         GNC       0.90      0.76      0.83        50\n         GPB       0.91      0.82      0.86        50\n         GPC       0.76      0.88      0.81        50\n\n    accuracy                           0.85       200\n   macro avg       0.86      0.85      0.85       200\nweighted avg       0.86      0.85      0.85       200\n\nClass-wise Sensitivity (Recall): [0.94 0.76 0.82 0.88]\nClass-wise Specificity: [0.94666667 0.97333333 0.97333333 0.90666667]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}