{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10304622,"sourceType":"datasetVersion","datasetId":6378605}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\n\nfrom collections import defaultdict\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport timm\n\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, classification_report, roc_auc_score, recall_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Check for CUDA device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(DEVICE)\n\nSEED = 16\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:09:20.091492Z","iopub.execute_input":"2025-01-19T06:09:20.091877Z","iopub.status.idle":"2025-01-19T06:09:27.823733Z","shell.execute_reply.started":"2025-01-19T06:09:20.091842Z","shell.execute_reply":"2025-01-19T06:09:27.822918Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Parameters\ndataset_path = '/kaggle/input/two-stage-dataset/Train_2Stage/BvsC'\nmodel_name = 'tiny_vit_21m_512.dist_in22k_ft_in1k'\nnum_epochs = 10\nnum_classes = 2\nbatch_size = 14\nk_folds = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:09:27.824733Z","iopub.execute_input":"2025-01-19T06:09:27.825234Z","iopub.status.idle":"2025-01-19T06:09:27.828855Z","shell.execute_reply.started":"2025-01-19T06:09:27.825210Z","shell.execute_reply":"2025-01-19T06:09:27.828112Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_sample_count(folder_path):\n    output = {}\n    classes = os.listdir(folder_path)\n    for i in classes:\n        output[i] = len(os.listdir(f'{folder_path}/{i}'))\n    return output\n\nprint(get_sample_count(dataset_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T06:09:27.830179Z","iopub.execute_input":"2025-01-19T06:09:27.830464Z","iopub.status.idle":"2025-01-19T06:09:27.879496Z","shell.execute_reply.started":"2025-01-19T06:09:27.830440Z","shell.execute_reply":"2025-01-19T06:09:27.878809Z"}},"outputs":[{"name":"stdout","text":"{'B': 1552, 'C': 1705}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# GET MODEL INFO\nm = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\nmodel_info = m.default_cfg\ndel m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T15:56:17.736930Z","iopub.execute_input":"2024-12-26T15:56:17.737237Z","iopub.status.idle":"2024-12-26T15:56:21.695092Z","shell.execute_reply.started":"2024-12-26T15:56:17.737210Z","shell.execute_reply":"2024-12-26T15:56:21.694436Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/85.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68993d74bb242feaf7b988d28ad39f0"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"input_shape = model_info['input_size'][1:]\ntransform_mean = model_info['mean']\ntransform_std = model_info['std']\n\nprint(f\"USING MODEL ARCHITECTURE {model_info['architecture']} \")\nprint(f\"INPUT SHAPE = {input_shape}\")\nprint(f\"       MEAN = {transform_mean}\")\nprint(f\"        STD = {transform_std}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T15:56:21.696106Z","iopub.execute_input":"2024-12-26T15:56:21.696355Z","iopub.status.idle":"2024-12-26T15:56:21.701024Z","shell.execute_reply.started":"2024-12-26T15:56:21.696334Z","shell.execute_reply":"2024-12-26T15:56:21.700194Z"}},"outputs":[{"name":"stdout","text":"USING MODEL ARCHITECTURE tiny_vit_21m_512 \nINPUT SHAPE = (512, 512)\n       MEAN = (0.485, 0.456, 0.406)\n        STD = (0.229, 0.224, 0.225)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"transform = transforms.Compose([\n        transforms.Resize(input_shape),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=transform_mean, std=transform_std)\n    ])\n\ndataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n\nprint(\"CLASS MAPPING\")\nprint(dataset.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T15:56:21.702399Z","iopub.execute_input":"2024-12-26T15:56:21.702690Z","iopub.status.idle":"2024-12-26T15:56:24.013831Z","shell.execute_reply.started":"2024-12-26T15:56:21.702659Z","shell.execute_reply":"2024-12-26T15:56:24.013056Z"}},"outputs":[{"name":"stdout","text":"CLASS MAPPING\n{'B': 0, 'C': 1}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n\nhistory = []\n\n# K-Fold Loop\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset, dataset.targets)):\n    print('\\n--------------------------------')\n    print(f'FOLD {fold+1}/{k_folds}')\n    print('--------------------------------')\n\n    # Create data loaders for this fold\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n\n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler, num_workers=4)\n    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler, num_workers=4)\n\n    # Initialize the model for this fold\n    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\n    # Add sigmoid activation for BCE Loss\n    model = nn.Sequential(\n        model,\n        nn.Sigmoid()\n    )\n    model = model.to(DEVICE)\n\n    # Change to BCE Loss\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    # Variables to keep track of the best model for this fold\n    best_val_acc = -999\n    best_epoch = 0\n    best_model_wts = None\n    best_preds = {\n        'labels':None,\n        'preds':None\n    }\n\n    # Model training loop START\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        model.train()\n        running_loss = 0.0\n\n        # Training\n        for inputs, labels in train_loader:\n            inputs = inputs.to(DEVICE)\n            # Convert labels to one-hot encoding and float type for BCE\n            labels_one_hot = torch.zeros(labels.size(0), num_classes)\n            labels_one_hot.scatter_(1, labels.unsqueeze(1), 1)\n            labels_one_hot = labels_one_hot.to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels_one_hot)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        all_labels = []\n        all_preds = []\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(DEVICE)\n                # Convert labels to one-hot encoding and float type for BCE\n                labels_one_hot = torch.zeros(labels.size(0), num_classes)\n                labels_one_hot.scatter_(1, labels.unsqueeze(1), 1)\n                labels_one_hot = labels_one_hot.to(DEVICE)\n                \n                outputs = model(inputs)\n                loss = criterion(outputs, labels_one_hot)\n                val_loss += loss.item()\n\n                # Convert probabilities to predictions\n                predicted = torch.max(outputs, 1)[1]\n                total += labels.size(0)\n                correct += (predicted.cpu() == labels).sum().item()\n                \n                all_labels.extend(labels.numpy())\n                all_preds.extend(predicted.cpu().numpy())\n\n        val_acc = correct / total\n\n        print(f'Loss: {running_loss/len(train_loader)} , Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {val_acc}')\n\n        # Always keep track of the best model and best output\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch + 1\n            best_model_wts = model.state_dict()\n            best_preds = {\n                'labels': all_labels,\n                'preds': all_preds\n            }\n\n    # TRAIN LOOP ENDS HERE\n    all_labels = best_preds['labels']\n    all_preds = best_preds['preds']\n    \n    # Calculate confusion matrix\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    \n    # Sensitivity, Specificity calculation\n    sensitivity = []\n    specificity = []\n    \n    for i in range(num_classes):\n        tp = conf_matrix[i, i]\n        fn = sum(conf_matrix[i, :]) - tp\n        fp = sum(conf_matrix[:, i]) - tp\n        tn = conf_matrix.sum() - (tp + fp + fn)\n\n        sensitivity.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='macro')\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    recall = recall_score(all_labels, all_preds, average='macro')\n\n    best_hist = { 'Accuracy': accuracy, 'Precision': precision, 'F1 Score': f1, 'recall': recall }\n\n    for class_name in dataset.class_to_idx:\n        best_hist[f'{class_name} Sensitivity'] = sensitivity[dataset.class_to_idx[class_name]]\n        best_hist[f'{class_name} Specificity'] = specificity[dataset.class_to_idx[class_name]]\n\n    print()\n    for key, value in best_hist.items():\n        print(f'{key}: {value}')\n\n    print()\n    # get classification report\n    print(classification_report(all_labels, all_preds, target_names=[i for i in dataset.class_to_idx]))\n    print()\n\n    # Save model to disk\n    print(f'Best Epoch for Fold {fold+1}: {best_epoch} with Validation Accuracy: {best_val_acc}')\n    torch.save(best_model_wts, f'fold{fold+1}_model.pth')\n    \n    history.append(best_hist)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T15:56:24.014650Z","iopub.execute_input":"2024-12-26T15:56:24.014875Z","iopub.status.idle":"2024-12-26T18:35:46.189884Z","shell.execute_reply.started":"2024-12-26T15:56:24.014855Z","shell.execute_reply":"2024-12-26T18:35:46.188750Z"}},"outputs":[{"name":"stdout","text":"\n--------------------------------\nFOLD 1/5\n--------------------------------\nEpoch 1/10\nLoss: 0.5604024905571963 , Val Loss: 0.41286928571285086, Val Accuracy: 0.8343558282208589\nEpoch 2/10\nLoss: 0.3754924373990074 , Val Loss: 0.3697203879660748, Val Accuracy: 0.8266871165644172\nEpoch 3/10\nLoss: 0.304266178671689 , Val Loss: 0.2768536056292818, Val Accuracy: 0.8803680981595092\nEpoch 4/10\nLoss: 0.26691465531998776 , Val Loss: 0.2465699036070641, Val Accuracy: 0.9003067484662577\nEpoch 5/10\nLoss: 0.24850142717281765 , Val Loss: 0.2540544186659316, Val Accuracy: 0.8849693251533742\nEpoch 6/10\nLoss: 0.2074119823780608 , Val Loss: 0.25150415903710305, Val Accuracy: 0.8880368098159509\nEpoch 7/10\nLoss: 0.19895353383399586 , Val Loss: 0.23467337205371958, Val Accuracy: 0.8941717791411042\nEpoch 8/10\nLoss: 0.2046414692014615 , Val Loss: 0.20743345683242412, Val Accuracy: 0.9141104294478528\nEpoch 9/10\nLoss: 0.15713811286769927 , Val Loss: 0.2003417765682048, Val Accuracy: 0.9202453987730062\nEpoch 10/10\nLoss: 0.16059614143229742 , Val Loss: 0.21479757260927496, Val Accuracy: 0.9187116564417178\n\nAccuracy: 0.9202453987730062\nPrecision: 0.9245980445670181\nF1 Score: 0.919609237919097\nrecall: 0.9180960104100857\nB Sensitivity: 0.8713826366559485\nB Specificity: 0.9648093841642229\nC Sensitivity: 0.9648093841642229\nC Specificity: 0.8713826366559485\n\n              precision    recall  f1-score   support\n\n           B       0.96      0.87      0.91       311\n           C       0.89      0.96      0.93       341\n\n    accuracy                           0.92       652\n   macro avg       0.92      0.92      0.92       652\nweighted avg       0.92      0.92      0.92       652\n\n\nBest Epoch for Fold 1: 9 with Validation Accuracy: 0.9202453987730062\n\n--------------------------------\nFOLD 2/5\n--------------------------------\nEpoch 1/10\nLoss: 0.5425569399155398 , Val Loss: 0.4766858014020514, Val Accuracy: 0.7822085889570553\nEpoch 2/10\nLoss: 0.37440175467474573 , Val Loss: 0.3297194220284198, Val Accuracy: 0.8665644171779141\nEpoch 3/10\nLoss: 0.2952236865452904 , Val Loss: 0.2222734806226923, Val Accuracy: 0.9217791411042945\nEpoch 4/10\nLoss: 0.25739850109831536 , Val Loss: 0.23859136932073755, Val Accuracy: 0.9202453987730062\nEpoch 5/10\nLoss: 0.24972166802315787 , Val Loss: 0.21771867755562702, Val Accuracy: 0.9217791411042945\nEpoch 6/10\nLoss: 0.2132684699194954 , Val Loss: 0.1900032394506196, Val Accuracy: 0.9340490797546013\nEpoch 7/10\nLoss: 0.1998090487730535 , Val Loss: 0.16489397667031339, Val Accuracy: 0.9294478527607362\nEpoch 8/10\nLoss: 0.184714223402627 , Val Loss: 0.1615613948633062, Val Accuracy: 0.9447852760736196\nEpoch 9/10\nLoss: 0.17592110309530706 , Val Loss: 0.18939199615666208, Val Accuracy: 0.9248466257668712\nEpoch 10/10\nLoss: 0.16996809932338522 , Val Loss: 0.13400643909389667, Val Accuracy: 0.9493865030674846\n\nAccuracy: 0.9493865030674846\nPrecision: 0.9539574402939375\nF1 Score: 0.9489966931765654\nrecall: 0.9473696617665086\nB Sensitivity: 0.9035369774919614\nB Specificity: 0.9912023460410557\nC Sensitivity: 0.9912023460410557\nC Specificity: 0.9035369774919614\n\n              precision    recall  f1-score   support\n\n           B       0.99      0.90      0.94       311\n           C       0.92      0.99      0.95       341\n\n    accuracy                           0.95       652\n   macro avg       0.95      0.95      0.95       652\nweighted avg       0.95      0.95      0.95       652\n\n\nBest Epoch for Fold 2: 10 with Validation Accuracy: 0.9493865030674846\n\n--------------------------------\nFOLD 3/5\n--------------------------------\nEpoch 1/10\nLoss: 0.5591388015664197 , Val Loss: 0.38557468322997396, Val Accuracy: 0.8494623655913979\nEpoch 2/10\nLoss: 0.3669850252528879 , Val Loss: 0.32315764005513903, Val Accuracy: 0.858678955453149\nEpoch 3/10\nLoss: 0.3050611441068471 , Val Loss: 0.26421580114897264, Val Accuracy: 0.9001536098310292\nEpoch 4/10\nLoss: 0.2692653802945652 , Val Loss: 0.24298814700004903, Val Accuracy: 0.8940092165898618\nEpoch 5/10\nLoss: 0.2292396321813053 , Val Loss: 0.23428362330540697, Val Accuracy: 0.9032258064516129\nEpoch 6/10\nLoss: 0.20451456405979426 , Val Loss: 0.25870469386907335, Val Accuracy: 0.8786482334869432\nEpoch 7/10\nLoss: 0.20575937861825694 , Val Loss: 0.23360518037163197, Val Accuracy: 0.9139784946236559\nEpoch 8/10\nLoss: 0.18669666764251688 , Val Loss: 0.18476907413532126, Val Accuracy: 0.9339477726574501\nEpoch 9/10\nLoss: 0.15983535611583666 , Val Loss: 0.24225819653811606, Val Accuracy: 0.9047619047619048\nEpoch 10/10\nLoss: 0.15351100008122942 , Val Loss: 0.21428550117986, Val Accuracy: 0.9231950844854071\n\nAccuracy: 0.9339477726574501\nPrecision: 0.9337015009912206\nF1 Score: 0.9338853710784811\nrecall: 0.9346041055718475\nB Sensitivity: 0.9483870967741935\nB Specificity: 0.9208211143695014\nC Sensitivity: 0.9208211143695014\nC Specificity: 0.9483870967741935\n\n              precision    recall  f1-score   support\n\n           B       0.92      0.95      0.93       310\n           C       0.95      0.92      0.94       341\n\n    accuracy                           0.93       651\n   macro avg       0.93      0.93      0.93       651\nweighted avg       0.93      0.93      0.93       651\n\n\nBest Epoch for Fold 3: 8 with Validation Accuracy: 0.9339477726574501\n\n--------------------------------\nFOLD 4/5\n--------------------------------\nEpoch 1/10\nLoss: 0.5634802547208766 , Val Loss: 0.41740387456214173, Val Accuracy: 0.8294930875576036\nEpoch 2/10\nLoss: 0.3557318815174587 , Val Loss: 0.29440330650578156, Val Accuracy: 0.8847926267281107\nEpoch 3/10\nLoss: 0.28957138975514446 , Val Loss: 0.24989645373313984, Val Accuracy: 0.9109062980030722\nEpoch 4/10\nLoss: 0.2646819151101265 , Val Loss: 0.25497880617671825, Val Accuracy: 0.8970814132104454\nEpoch 5/10\nLoss: 0.21917673744221422 , Val Loss: 0.23232340242000335, Val Accuracy: 0.9078341013824884\nEpoch 6/10\nLoss: 0.18950600106826918 , Val Loss: 0.20867180483455353, Val Accuracy: 0.9185867895545314\nEpoch 7/10\nLoss: 0.1913511942036171 , Val Loss: 0.23431959205326883, Val Accuracy: 0.9078341013824884\nEpoch 8/10\nLoss: 0.1723018738735009 , Val Loss: 0.2335209202695083, Val Accuracy: 0.9170506912442397\nEpoch 9/10\nLoss: 0.1482919176223683 , Val Loss: 0.24741262617580434, Val Accuracy: 0.9047619047619048\nEpoch 10/10\nLoss: 0.16377041593472907 , Val Loss: 0.20527259615111224, Val Accuracy: 0.9308755760368663\n\nAccuracy: 0.9308755760368663\nPrecision: 0.9310859471240406\nF1 Score: 0.9306635424432848\nrecall: 0.9303519061583578\nB Sensitivity: 0.9193548387096774\nB Specificity: 0.9413489736070382\nC Sensitivity: 0.9413489736070382\nC Specificity: 0.9193548387096774\n\n              precision    recall  f1-score   support\n\n           B       0.93      0.92      0.93       310\n           C       0.93      0.94      0.93       341\n\n    accuracy                           0.93       651\n   macro avg       0.93      0.93      0.93       651\nweighted avg       0.93      0.93      0.93       651\n\n\nBest Epoch for Fold 4: 10 with Validation Accuracy: 0.9308755760368663\n\n--------------------------------\nFOLD 5/5\n--------------------------------\nEpoch 1/10\nLoss: 0.5437355867841027 , Val Loss: 0.40422505710987333, Val Accuracy: 0.8448540706605223\nEpoch 2/10\nLoss: 0.3765771763528732 , Val Loss: 0.29803971819421077, Val Accuracy: 0.8755760368663594\nEpoch 3/10\nLoss: 0.30871197461763167 , Val Loss: 0.23617189250727919, Val Accuracy: 0.9170506912442397\nEpoch 4/10\nLoss: 0.27241647576186107 , Val Loss: 0.25696738317925877, Val Accuracy: 0.8986175115207373\nEpoch 5/10\nLoss: 0.22819785373335216 , Val Loss: 0.2260224636485602, Val Accuracy: 0.9062980030721967\nEpoch 6/10\nLoss: 0.20272415634344287 , Val Loss: 0.20364101540218008, Val Accuracy: 0.9247311827956989\nEpoch 7/10\nLoss: 0.20119263639145676 , Val Loss: 0.19502532133396636, Val Accuracy: 0.9231950844854071\nEpoch 8/10\nLoss: 0.1862757283955175 , Val Loss: 0.20744838157391293, Val Accuracy: 0.9185867895545314\nEpoch 9/10\nLoss: 0.16454167525597754 , Val Loss: 0.26966154414843374, Val Accuracy: 0.901689708141321\nEpoch 10/10\nLoss: 0.16601255147115274 , Val Loss: 0.2186873143618094, Val Accuracy: 0.9231950844854071\n\nAccuracy: 0.9247311827956989\nPrecision: 0.9247500189379592\nF1 Score: 0.9245253108400667\nrecall: 0.9243401759530792\nB Sensitivity: 0.9161290322580645\nB Specificity: 0.9325513196480938\nC Sensitivity: 0.9325513196480938\nC Specificity: 0.9161290322580645\n\n              precision    recall  f1-score   support\n\n           B       0.93      0.92      0.92       310\n           C       0.92      0.93      0.93       341\n\n    accuracy                           0.92       651\n   macro avg       0.92      0.92      0.92       651\nweighted avg       0.92      0.92      0.92       651\n\n\nBest Epoch for Fold 5: 6 with Validation Accuracy: 0.9247311827956989\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:36:25.101585Z","iopub.execute_input":"2024-12-26T18:36:25.102037Z","iopub.status.idle":"2024-12-26T18:36:25.110818Z","shell.execute_reply.started":"2024-12-26T18:36:25.101987Z","shell.execute_reply":"2024-12-26T18:36:25.109796Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[{'Accuracy': 0.9202453987730062,\n  'Precision': 0.9245980445670181,\n  'F1 Score': 0.919609237919097,\n  'recall': 0.9180960104100857,\n  'B Sensitivity': 0.8713826366559485,\n  'B Specificity': 0.9648093841642229,\n  'C Sensitivity': 0.9648093841642229,\n  'C Specificity': 0.8713826366559485},\n {'Accuracy': 0.9493865030674846,\n  'Precision': 0.9539574402939375,\n  'F1 Score': 0.9489966931765654,\n  'recall': 0.9473696617665086,\n  'B Sensitivity': 0.9035369774919614,\n  'B Specificity': 0.9912023460410557,\n  'C Sensitivity': 0.9912023460410557,\n  'C Specificity': 0.9035369774919614},\n {'Accuracy': 0.9339477726574501,\n  'Precision': 0.9337015009912206,\n  'F1 Score': 0.9338853710784811,\n  'recall': 0.9346041055718475,\n  'B Sensitivity': 0.9483870967741935,\n  'B Specificity': 0.9208211143695014,\n  'C Sensitivity': 0.9208211143695014,\n  'C Specificity': 0.9483870967741935},\n {'Accuracy': 0.9308755760368663,\n  'Precision': 0.9310859471240406,\n  'F1 Score': 0.9306635424432848,\n  'recall': 0.9303519061583578,\n  'B Sensitivity': 0.9193548387096774,\n  'B Specificity': 0.9413489736070382,\n  'C Sensitivity': 0.9413489736070382,\n  'C Specificity': 0.9193548387096774},\n {'Accuracy': 0.9247311827956989,\n  'Precision': 0.9247500189379592,\n  'F1 Score': 0.9245253108400667,\n  'recall': 0.9243401759530792,\n  'B Sensitivity': 0.9161290322580645,\n  'B Specificity': 0.9325513196480938,\n  'C Sensitivity': 0.9325513196480938,\n  'C Specificity': 0.9161290322580645}]"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"!zip BvsC.zip ./*.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:36:57.452035Z","iopub.execute_input":"2024-12-26T18:36:57.452395Z","iopub.status.idle":"2024-12-26T18:37:18.949379Z","shell.execute_reply.started":"2024-12-26T18:36:57.452365Z","shell.execute_reply":"2024-12-26T18:37:18.948549Z"}},"outputs":[{"name":"stdout","text":"  adding: fold1_model.pth (deflated 7%)\n  adding: fold2_model.pth (deflated 7%)\n  adding: fold3_model.pth (deflated 7%)\n  adding: fold4_model.pth (deflated 7%)\n  adding: fold5_model.pth (deflated 7%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"hist_df = pd.DataFrame(history)\nhist_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:37:28.860096Z","iopub.execute_input":"2024-12-26T18:37:28.860447Z","iopub.status.idle":"2024-12-26T18:37:28.885807Z","shell.execute_reply.started":"2024-12-26T18:37:28.860419Z","shell.execute_reply":"2024-12-26T18:37:28.885010Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   Accuracy  Precision  F1 Score    recall  B Sensitivity  B Specificity  \\\n0  0.920245   0.924598  0.919609  0.918096       0.871383       0.964809   \n1  0.949387   0.953957  0.948997  0.947370       0.903537       0.991202   \n2  0.933948   0.933702  0.933885  0.934604       0.948387       0.920821   \n3  0.930876   0.931086  0.930664  0.930352       0.919355       0.941349   \n4  0.924731   0.924750  0.924525  0.924340       0.916129       0.932551   \n\n   C Sensitivity  C Specificity  \n0       0.964809       0.871383  \n1       0.991202       0.903537  \n2       0.920821       0.948387  \n3       0.941349       0.919355  \n4       0.932551       0.916129  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>F1 Score</th>\n      <th>recall</th>\n      <th>B Sensitivity</th>\n      <th>B Specificity</th>\n      <th>C Sensitivity</th>\n      <th>C Specificity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.920245</td>\n      <td>0.924598</td>\n      <td>0.919609</td>\n      <td>0.918096</td>\n      <td>0.871383</td>\n      <td>0.964809</td>\n      <td>0.964809</td>\n      <td>0.871383</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.949387</td>\n      <td>0.953957</td>\n      <td>0.948997</td>\n      <td>0.947370</td>\n      <td>0.903537</td>\n      <td>0.991202</td>\n      <td>0.991202</td>\n      <td>0.903537</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.933948</td>\n      <td>0.933702</td>\n      <td>0.933885</td>\n      <td>0.934604</td>\n      <td>0.948387</td>\n      <td>0.920821</td>\n      <td>0.920821</td>\n      <td>0.948387</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.930876</td>\n      <td>0.931086</td>\n      <td>0.930664</td>\n      <td>0.930352</td>\n      <td>0.919355</td>\n      <td>0.941349</td>\n      <td>0.941349</td>\n      <td>0.919355</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.924731</td>\n      <td>0.924750</td>\n      <td>0.924525</td>\n      <td>0.924340</td>\n      <td>0.916129</td>\n      <td>0.932551</td>\n      <td>0.932551</td>\n      <td>0.916129</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}