{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\armac\\.conda\\envs\\dev\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import timm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, classification_report, roc_auc_score, recall_score\n",
    "\n",
    "SEED = 16\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../DATA/Train_Val_set/Val'\n",
    "model_path = '../MODELS/VIT_DataSet01_M1/fold3_model.pth'\n",
    "\n",
    "# Parameters\n",
    "model_name = 'tiny_vit_21m_512.dist_in22k_ft_in1k'\n",
    "num_classes = 4\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING MODEL ARCHITECTURE tiny_vit_21m_512 \n",
      "INPUT SHAPE = (512, 512)\n",
      "       MEAN = (0.485, 0.456, 0.406)\n",
      "        STD = (0.229, 0.224, 0.225)\n"
     ]
    }
   ],
   "source": [
    "m = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\n",
    "model_info = m.default_cfg\n",
    "del m\n",
    "\n",
    "input_shape = model_info['input_size'][1:]\n",
    "transform_mean = model_info['mean']\n",
    "transform_std = model_info['std']\n",
    "\n",
    "print(f\"USING MODEL ARCHITECTURE {model_info['architecture']} \")\n",
    "print(f\"INPUT SHAPE = {input_shape}\")\n",
    "print(f\"       MEAN = {transform_mean}\")\n",
    "print(f\"        STD = {transform_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS MAPPING\n",
      "{'GNB': 0, 'GNC': 1, 'GPB': 2, 'GPC': 3}\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "    ])\n",
    "\n",
    "data_dataset = datasets.ImageFolder(root=dataset_path, transform=data_transform)\n",
    "data_loader = DataLoader(data_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"CLASS MAPPING\")\n",
    "print(data_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armac\\AppData\\Local\\Temp\\ipykernel_14284\\146030472.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(DEVICE)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TinyVit(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (conv1): ConvNorm(\n",
       "      (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act): GELU(approximate='none')\n",
       "    (conv2): ConvNorm(\n",
       "      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (blocks): Sequential(\n",
       "        (0): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TinyVitStage(\n",
       "      dim=192, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=192, num_heads=6, window_size=16, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=192, num_heads=6, window_size=16, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.027)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.027)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TinyVitStage(\n",
       "      dim=384, depth=6\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.045)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.045)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.064)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.064)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TinyVitBlock(\n",
       "          dim=384, num_heads=12, window_size=32, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.082)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.082)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TinyVitStage(\n",
       "      dim=576, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(384, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=576, num_heads=18, window_size=16, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=576, num_heads=18, window_size=16, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=576, out_features=2304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.3, inplace=False)\n",
       "            (fc2): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            (drop2): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((576,), eps=1e-05, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=576, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.3)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(DEVICE)))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\armac\\.conda\\envs\\dev\\lib\\site-packages\\timm\\models\\tiny_vit.py:234: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GNB       0.80      0.78      0.79        50\n",
      "         GNC       0.95      0.38      0.54        50\n",
      "         GPB       0.84      0.86      0.85        50\n",
      "         GPC       0.61      0.98      0.75        50\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.80      0.75      0.73       200\n",
      "weighted avg       0.80      0.75      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        for x in outputs: logits.append(x)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=[i for i in data_dataset.class_to_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GNB': 0, 'GNC': 1, 'GPB': 2, 'GPC': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7473, 0.1190, 0.0941, 0.0396],\n",
       "        [0.6064, 0.0090, 0.2838, 0.1009],\n",
       "        [0.8222, 0.0335, 0.0611, 0.0832],\n",
       "        [0.4364, 0.0147, 0.5044, 0.0445],\n",
       "        [0.9148, 0.0197, 0.0202, 0.0453],\n",
       "        [0.8382, 0.0509, 0.0950, 0.0159],\n",
       "        [0.6633, 0.1509, 0.1034, 0.0824],\n",
       "        [0.5562, 0.1232, 0.2418, 0.0788],\n",
       "        [0.7127, 0.0135, 0.2426, 0.0311],\n",
       "        [0.7020, 0.0209, 0.0753, 0.2018],\n",
       "        [0.6452, 0.1812, 0.0690, 0.1046],\n",
       "        [0.5278, 0.0101, 0.3119, 0.1503],\n",
       "        [0.3892, 0.0059, 0.3126, 0.2923],\n",
       "        [0.0448, 0.0135, 0.8953, 0.0465],\n",
       "        [0.7107, 0.0275, 0.1721, 0.0897],\n",
       "        [0.2538, 0.4197, 0.0655, 0.2610],\n",
       "        [0.2458, 0.0291, 0.5494, 0.1757],\n",
       "        [0.6288, 0.0088, 0.2062, 0.1562],\n",
       "        [0.8478, 0.1021, 0.0139, 0.0362],\n",
       "        [0.4775, 0.0220, 0.3278, 0.1727],\n",
       "        [0.1349, 0.0109, 0.4119, 0.4423],\n",
       "        [0.6149, 0.0106, 0.3388, 0.0357],\n",
       "        [0.1228, 0.0595, 0.3562, 0.4614],\n",
       "        [0.7719, 0.0497, 0.0758, 0.1025],\n",
       "        [0.3668, 0.0096, 0.5826, 0.0410],\n",
       "        [0.7306, 0.1269, 0.0754, 0.0670],\n",
       "        [0.8286, 0.0198, 0.1087, 0.0429],\n",
       "        [0.4309, 0.3009, 0.1109, 0.1573],\n",
       "        [0.4175, 0.0848, 0.3047, 0.1931],\n",
       "        [0.5694, 0.0112, 0.3599, 0.0594],\n",
       "        [0.5092, 0.0212, 0.2372, 0.2323],\n",
       "        [0.9000, 0.0148, 0.0616, 0.0236],\n",
       "        [0.1750, 0.0610, 0.6089, 0.1550],\n",
       "        [0.9495, 0.0169, 0.0177, 0.0160],\n",
       "        [0.7377, 0.0087, 0.2078, 0.0457],\n",
       "        [0.0253, 0.0049, 0.3309, 0.6389],\n",
       "        [0.7586, 0.0759, 0.0745, 0.0910],\n",
       "        [0.9089, 0.0116, 0.0403, 0.0393],\n",
       "        [0.4586, 0.0211, 0.3895, 0.1307],\n",
       "        [0.2060, 0.0334, 0.1232, 0.6374],\n",
       "        [0.6338, 0.0875, 0.1220, 0.1567],\n",
       "        [0.7897, 0.0618, 0.0393, 0.1092],\n",
       "        [0.7242, 0.0493, 0.0956, 0.1310],\n",
       "        [0.4420, 0.1226, 0.1661, 0.2693],\n",
       "        [0.5055, 0.0317, 0.2609, 0.2020],\n",
       "        [0.9386, 0.0136, 0.0308, 0.0169],\n",
       "        [0.9086, 0.0330, 0.0327, 0.0257],\n",
       "        [0.5515, 0.0060, 0.3867, 0.0558],\n",
       "        [0.2015, 0.0165, 0.5744, 0.2076],\n",
       "        [0.4924, 0.0849, 0.1668, 0.2559],\n",
       "        [0.0393, 0.9297, 0.0150, 0.0159],\n",
       "        [0.1249, 0.0303, 0.1576, 0.6872],\n",
       "        [0.5032, 0.2465, 0.0358, 0.2146],\n",
       "        [0.0697, 0.7887, 0.0622, 0.0794],\n",
       "        [0.3557, 0.1608, 0.1700, 0.3135],\n",
       "        [0.0788, 0.7580, 0.0525, 0.1107],\n",
       "        [0.0935, 0.6984, 0.0363, 0.1718],\n",
       "        [0.3615, 0.0147, 0.4454, 0.1783],\n",
       "        [0.0825, 0.8662, 0.0126, 0.0387],\n",
       "        [0.5749, 0.3624, 0.0240, 0.0388],\n",
       "        [0.0990, 0.0313, 0.0658, 0.8039],\n",
       "        [0.0651, 0.0078, 0.1046, 0.8225],\n",
       "        [0.3318, 0.3907, 0.0864, 0.1912],\n",
       "        [0.0219, 0.9489, 0.0103, 0.0189],\n",
       "        [0.2939, 0.0995, 0.1957, 0.4109],\n",
       "        [0.0398, 0.0119, 0.2006, 0.7477],\n",
       "        [0.4080, 0.0892, 0.0721, 0.4307],\n",
       "        [0.8471, 0.0207, 0.1023, 0.0298],\n",
       "        [0.0741, 0.7713, 0.0365, 0.1182],\n",
       "        [0.1150, 0.0763, 0.1157, 0.6930],\n",
       "        [0.0856, 0.0447, 0.0989, 0.7708],\n",
       "        [0.0233, 0.0129, 0.0256, 0.9382],\n",
       "        [0.3109, 0.0731, 0.1605, 0.4554],\n",
       "        [0.0480, 0.9192, 0.0158, 0.0170],\n",
       "        [0.0186, 0.9612, 0.0067, 0.0134],\n",
       "        [0.0503, 0.0174, 0.1296, 0.8027],\n",
       "        [0.3174, 0.6108, 0.0189, 0.0528],\n",
       "        [0.0297, 0.9559, 0.0070, 0.0074],\n",
       "        [0.2014, 0.6664, 0.0382, 0.0940],\n",
       "        [0.2136, 0.1338, 0.0614, 0.5912],\n",
       "        [0.6921, 0.0814, 0.0489, 0.1775],\n",
       "        [0.0241, 0.9380, 0.0179, 0.0200],\n",
       "        [0.0176, 0.0037, 0.0148, 0.9639],\n",
       "        [0.3801, 0.0100, 0.0946, 0.5152],\n",
       "        [0.2382, 0.5308, 0.0242, 0.2069],\n",
       "        [0.0166, 0.9599, 0.0088, 0.0147],\n",
       "        [0.0204, 0.9245, 0.0145, 0.0406],\n",
       "        [0.5210, 0.0696, 0.2104, 0.1990],\n",
       "        [0.0911, 0.0966, 0.0489, 0.7634],\n",
       "        [0.3200, 0.0128, 0.0704, 0.5968],\n",
       "        [0.7717, 0.0420, 0.1134, 0.0729],\n",
       "        [0.4072, 0.0688, 0.0815, 0.4425],\n",
       "        [0.2128, 0.6985, 0.0339, 0.0547],\n",
       "        [0.0170, 0.9604, 0.0092, 0.0133],\n",
       "        [0.1986, 0.0304, 0.2878, 0.4832],\n",
       "        [0.5432, 0.0192, 0.0851, 0.3525],\n",
       "        [0.0158, 0.0034, 0.1131, 0.8678],\n",
       "        [0.1498, 0.0733, 0.0304, 0.7465],\n",
       "        [0.2578, 0.0935, 0.1235, 0.5252],\n",
       "        [0.2465, 0.1377, 0.0880, 0.5278],\n",
       "        [0.0192, 0.0050, 0.4943, 0.4814],\n",
       "        [0.6007, 0.0286, 0.3393, 0.0314],\n",
       "        [0.0136, 0.0021, 0.6460, 0.3383],\n",
       "        [0.0458, 0.0043, 0.9080, 0.0419],\n",
       "        [0.0522, 0.0045, 0.7962, 0.1471],\n",
       "        [0.0137, 0.0090, 0.6104, 0.3669],\n",
       "        [0.0553, 0.0081, 0.8867, 0.0500],\n",
       "        [0.0320, 0.0050, 0.9020, 0.0611],\n",
       "        [0.0277, 0.0080, 0.6004, 0.3639],\n",
       "        [0.1053, 0.0239, 0.7989, 0.0719],\n",
       "        [0.0119, 0.0076, 0.9340, 0.0465],\n",
       "        [0.1626, 0.0063, 0.4361, 0.3949],\n",
       "        [0.0903, 0.0145, 0.6211, 0.2740],\n",
       "        [0.0611, 0.0082, 0.8950, 0.0357],\n",
       "        [0.0461, 0.0045, 0.8995, 0.0500],\n",
       "        [0.1528, 0.0149, 0.6809, 0.1513],\n",
       "        [0.2227, 0.0064, 0.5689, 0.2021],\n",
       "        [0.2009, 0.0455, 0.5181, 0.2354],\n",
       "        [0.0322, 0.0026, 0.8606, 0.1045],\n",
       "        [0.0232, 0.0030, 0.9037, 0.0702],\n",
       "        [0.0111, 0.0079, 0.9358, 0.0452],\n",
       "        [0.0173, 0.0027, 0.3379, 0.6421],\n",
       "        [0.1071, 0.0130, 0.7991, 0.0807],\n",
       "        [0.0726, 0.0067, 0.5321, 0.3886],\n",
       "        [0.0618, 0.0106, 0.5597, 0.3680],\n",
       "        [0.0417, 0.0057, 0.2464, 0.7062],\n",
       "        [0.0119, 0.0021, 0.8770, 0.1089],\n",
       "        [0.1869, 0.0179, 0.5762, 0.2190],\n",
       "        [0.0232, 0.0042, 0.7670, 0.2056],\n",
       "        [0.0919, 0.0030, 0.3455, 0.5596],\n",
       "        [0.3481, 0.0375, 0.5426, 0.0718],\n",
       "        [0.0223, 0.0039, 0.9457, 0.0281],\n",
       "        [0.2406, 0.0108, 0.5219, 0.2268],\n",
       "        [0.0150, 0.0185, 0.3017, 0.6648],\n",
       "        [0.0573, 0.3108, 0.5418, 0.0900],\n",
       "        [0.0500, 0.0028, 0.9120, 0.0352],\n",
       "        [0.0417, 0.0074, 0.9222, 0.0288],\n",
       "        [0.0322, 0.0040, 0.6530, 0.3108],\n",
       "        [0.2296, 0.0131, 0.6471, 0.1102],\n",
       "        [0.0810, 0.0037, 0.8612, 0.0541],\n",
       "        [0.0976, 0.0042, 0.8467, 0.0515],\n",
       "        [0.1080, 0.0152, 0.5674, 0.3094],\n",
       "        [0.0250, 0.0048, 0.8591, 0.1110],\n",
       "        [0.0448, 0.0164, 0.8362, 0.1026],\n",
       "        [0.0531, 0.0080, 0.8638, 0.0751],\n",
       "        [0.1056, 0.0754, 0.6249, 0.1940],\n",
       "        [0.0915, 0.0393, 0.2182, 0.6511],\n",
       "        [0.1122, 0.0264, 0.7673, 0.0941],\n",
       "        [0.4317, 0.0242, 0.3794, 0.1647],\n",
       "        [0.0890, 0.0082, 0.7757, 0.1270],\n",
       "        [0.0084, 0.0029, 0.0613, 0.9274],\n",
       "        [0.0760, 0.0044, 0.1159, 0.8036],\n",
       "        [0.0320, 0.0032, 0.0686, 0.8962],\n",
       "        [0.2644, 0.1447, 0.0375, 0.5535],\n",
       "        [0.0311, 0.0072, 0.0290, 0.9327],\n",
       "        [0.0093, 0.0047, 0.1148, 0.8712],\n",
       "        [0.0092, 0.0156, 0.0947, 0.8805],\n",
       "        [0.0201, 0.0029, 0.0708, 0.9061],\n",
       "        [0.0366, 0.0040, 0.1406, 0.8187],\n",
       "        [0.0089, 0.0029, 0.1519, 0.8363],\n",
       "        [0.0143, 0.0614, 0.0969, 0.8273],\n",
       "        [0.0218, 0.0133, 0.0728, 0.8921],\n",
       "        [0.0084, 0.0054, 0.0194, 0.9668],\n",
       "        [0.1007, 0.0087, 0.0861, 0.8046],\n",
       "        [0.0190, 0.0088, 0.0937, 0.8785],\n",
       "        [0.0445, 0.0390, 0.0713, 0.8451],\n",
       "        [0.0875, 0.0104, 0.1221, 0.7800],\n",
       "        [0.3335, 0.0275, 0.1929, 0.4461],\n",
       "        [0.0216, 0.0080, 0.0905, 0.8800],\n",
       "        [0.0298, 0.0069, 0.3296, 0.6337],\n",
       "        [0.0357, 0.0092, 0.3748, 0.5803],\n",
       "        [0.0868, 0.0089, 0.0990, 0.8054],\n",
       "        [0.0635, 0.0083, 0.0300, 0.8983],\n",
       "        [0.0457, 0.0044, 0.0229, 0.9270],\n",
       "        [0.1298, 0.0277, 0.1265, 0.7161],\n",
       "        [0.1737, 0.0319, 0.4422, 0.3522],\n",
       "        [0.0091, 0.0025, 0.0995, 0.8890],\n",
       "        [0.0276, 0.0045, 0.0887, 0.8792],\n",
       "        [0.0192, 0.0024, 0.1154, 0.8631],\n",
       "        [0.0069, 0.0020, 0.0223, 0.9688],\n",
       "        [0.0470, 0.0019, 0.1484, 0.8028],\n",
       "        [0.0142, 0.0152, 0.1080, 0.8627],\n",
       "        [0.0252, 0.0404, 0.0559, 0.8785],\n",
       "        [0.0168, 0.0106, 0.1479, 0.8246],\n",
       "        [0.0765, 0.0080, 0.0718, 0.8437],\n",
       "        [0.0143, 0.0110, 0.0759, 0.8988],\n",
       "        [0.0157, 0.0085, 0.0339, 0.9419],\n",
       "        [0.0355, 0.0078, 0.2176, 0.7391],\n",
       "        [0.0887, 0.0053, 0.2531, 0.6529],\n",
       "        [0.1803, 0.0199, 0.2017, 0.5981],\n",
       "        [0.0334, 0.0022, 0.2000, 0.7643],\n",
       "        [0.0376, 0.0191, 0.0525, 0.8908],\n",
       "        [0.0174, 0.0031, 0.0200, 0.9596],\n",
       "        [0.0565, 0.0067, 0.0851, 0.8516],\n",
       "        [0.0124, 0.0168, 0.0592, 0.9116],\n",
       "        [0.0163, 0.0018, 0.0726, 0.9093],\n",
       "        [0.0669, 0.0027, 0.1527, 0.7777],\n",
       "        [0.0448, 0.0129, 0.0744, 0.8679],\n",
       "        [0.0237, 0.0062, 0.0595, 0.9106],\n",
       "        [0.0121, 0.0065, 0.2103, 0.7711]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert logits to probabilities\n",
    "logits = torch.stack(logits)\n",
    "\n",
    "logits = nn.Softmax(dim=1)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB, GNC, GPB, GPC, label, pred\n",
      "0.7473011612892151, 0.11903907358646393, 0.09406524151563644, 0.03959449380636215, 0, 0\n",
      "0.6064049005508423, 0.008950510062277317, 0.28375568985939026, 0.10088887065649033, 0, 0\n",
      "0.8222436904907227, 0.03349340707063675, 0.061087556183338165, 0.08317539840936661, 0, 0\n",
      "0.43639472126960754, 0.014715207740664482, 0.504367470741272, 0.04452258348464966, 0, 2\n",
      "0.9148030877113342, 0.019677210599184036, 0.020247962325811386, 0.04527170956134796, 0, 0\n",
      "0.8382396697998047, 0.05085292086005211, 0.09501572698354721, 0.015891732648015022, 0, 0\n",
      "0.6632636785507202, 0.150929793715477, 0.10344664752483368, 0.0823599249124527, 0, 0\n",
      "0.5562048554420471, 0.12318769842386246, 0.24181540310382843, 0.07879205793142319, 0, 0\n",
      "0.7127335667610168, 0.013537299819290638, 0.24263426661491394, 0.031094852834939957, 0, 0\n",
      "0.7020370364189148, 0.020871849730610847, 0.07534057646989822, 0.20175053179264069, 0, 0\n",
      "0.6452324986457825, 0.18120619654655457, 0.06895191222429276, 0.10460937023162842, 0, 0\n",
      "0.5277611017227173, 0.010083272121846676, 0.31187352538108826, 0.15028204023838043, 0, 0\n",
      "0.3892328143119812, 0.005937913432717323, 0.3125654458999634, 0.29226383566856384, 0, 0\n",
      "0.04475890472531319, 0.013514738529920578, 0.8952615857124329, 0.04646480828523636, 0, 2\n",
      "0.7107095718383789, 0.02753945253789425, 0.17205582559108734, 0.08969513326883316, 0, 0\n",
      "0.2537834644317627, 0.419731080532074, 0.06548572331666946, 0.26099979877471924, 0, 1\n",
      "0.24581333994865417, 0.02908780425786972, 0.5494256615638733, 0.17567317187786102, 0, 2\n",
      "0.6288314461708069, 0.008773403242230415, 0.20622111856937408, 0.15617400407791138, 0, 0\n",
      "0.8478368520736694, 0.10205745697021484, 0.01392444595694542, 0.0361812524497509, 0, 0\n",
      "0.4775330424308777, 0.021973788738250732, 0.32777974009513855, 0.17271339893341064, 0, 0\n",
      "0.13493165373802185, 0.010885635390877724, 0.41189420223236084, 0.44228848814964294, 0, 3\n",
      "0.6149366497993469, 0.01059760618954897, 0.33875584602355957, 0.03570991009473801, 0, 0\n",
      "0.12284597754478455, 0.059522923082113266, 0.3561888635158539, 0.461442232131958, 0, 3\n",
      "0.7718762159347534, 0.04973325505852699, 0.07584848254919052, 0.10254205018281937, 0, 0\n",
      "0.3668270409107208, 0.00961175188422203, 0.5825909972190857, 0.040970172733068466, 0, 2\n",
      "0.7305606007575989, 0.12694859504699707, 0.07544736564159393, 0.06704340875148773, 0, 0\n",
      "0.8286101222038269, 0.019830837845802307, 0.10869336873292923, 0.04286567121744156, 0, 0\n",
      "0.4309456944465637, 0.30091148614883423, 0.11088213324546814, 0.1572606861591339, 0, 0\n",
      "0.4174518585205078, 0.08478141576051712, 0.3046797811985016, 0.19308693706989288, 0, 0\n",
      "0.5694390535354614, 0.011228312738239765, 0.35990607738494873, 0.059426482766866684, 0, 0\n",
      "0.509200930595398, 0.021228093653917313, 0.23722341656684875, 0.23234757781028748, 0, 0\n",
      "0.9000024795532227, 0.014847802929580212, 0.06158069893717766, 0.023569002747535706, 0, 0\n",
      "0.1750410497188568, 0.06102301552891731, 0.6089052557945251, 0.15503063797950745, 0, 2\n",
      "0.9494531154632568, 0.01685481145977974, 0.017658157274127007, 0.016033926978707314, 0, 0\n",
      "0.7377268075942993, 0.008700039237737656, 0.20782728493213654, 0.04574589431285858, 0, 0\n",
      "0.025329524651169777, 0.004880172200500965, 0.3309221863746643, 0.6388680934906006, 0, 3\n",
      "0.7585651874542236, 0.07585376501083374, 0.074531689286232, 0.0910494327545166, 0, 0\n",
      "0.9088776707649231, 0.011596431024372578, 0.040272731333971024, 0.03925319388508797, 0, 0\n",
      "0.45863938331604004, 0.02112802118062973, 0.38952192664146423, 0.130710631608963, 0, 0\n",
      "0.2059745490550995, 0.03343060612678528, 0.12315044552087784, 0.6374444365501404, 0, 3\n",
      "0.6337966322898865, 0.0874931588768959, 0.12202522158622742, 0.15668494999408722, 0, 0\n",
      "0.7896829843521118, 0.06177964806556702, 0.03931329771876335, 0.10922402888536453, 0, 0\n",
      "0.7241838574409485, 0.04925348609685898, 0.09555159509181976, 0.13101109862327576, 0, 0\n",
      "0.4419773519039154, 0.1226288378238678, 0.1660526841878891, 0.2693410813808441, 0, 0\n",
      "0.5054638385772705, 0.03168865293264389, 0.26088830828666687, 0.20195919275283813, 0, 0\n",
      "0.9386436939239502, 0.013622297905385494, 0.03080519661307335, 0.01692882739007473, 0, 0\n",
      "0.9085510969161987, 0.03304394334554672, 0.032717689871788025, 0.02568725310266018, 0, 0\n",
      "0.5515158176422119, 0.0060241627506911755, 0.38667505979537964, 0.05578496679663658, 0, 0\n",
      "0.201499342918396, 0.016547784209251404, 0.5743790864944458, 0.2075738161802292, 0, 2\n",
      "0.4924187660217285, 0.08486399054527283, 0.16677770018577576, 0.2559395730495453, 0, 0\n",
      "0.03933221474289894, 0.9296935200691223, 0.015030987560749054, 0.015943290665745735, 1, 1\n",
      "0.12488042563199997, 0.030320344492793083, 0.1575525999069214, 0.6872466802597046, 1, 3\n",
      "0.5031731128692627, 0.24647994339466095, 0.03576591983437538, 0.21458107233047485, 1, 0\n",
      "0.06970379501581192, 0.7886515259742737, 0.06219835951924324, 0.07944627851247787, 1, 1\n",
      "0.3557036221027374, 0.16080211102962494, 0.17002001404762268, 0.31347423791885376, 1, 0\n",
      "0.07877937704324722, 0.7579972743988037, 0.052520234137773514, 0.11070317775011063, 1, 1\n",
      "0.09349628537893295, 0.698378324508667, 0.03631000965833664, 0.17181545495986938, 1, 1\n",
      "0.36154884099960327, 0.014716074801981449, 0.4453858435153961, 0.1783493012189865, 1, 2\n",
      "0.08247289806604385, 0.866205096244812, 0.0126256812363863, 0.03869638219475746, 1, 1\n",
      "0.5748554468154907, 0.3623504042625427, 0.023983681574463844, 0.03881043940782547, 1, 0\n",
      "0.09896425902843475, 0.031337715685367584, 0.06583178788423538, 0.8038662672042847, 1, 3\n",
      "0.06509052962064743, 0.007764885667711496, 0.10461106896400452, 0.8225334882736206, 1, 3\n",
      "0.33177074790000916, 0.3906759023666382, 0.08639751374721527, 0.191155806183815, 1, 1\n",
      "0.02188396267592907, 0.9489371180534363, 0.010314454324543476, 0.01886448822915554, 1, 1\n",
      "0.29388466477394104, 0.09949100762605667, 0.19567802548408508, 0.4109462797641754, 1, 3\n",
      "0.039836518466472626, 0.011905516497790813, 0.2005990445613861, 0.747658908367157, 1, 3\n",
      "0.40797388553619385, 0.08917437493801117, 0.07210659980773926, 0.43074503540992737, 1, 3\n",
      "0.8471349477767944, 0.020696157589554787, 0.1023344174027443, 0.02983451820909977, 1, 0\n",
      "0.07405366003513336, 0.7712997794151306, 0.03647972643375397, 0.11816685646772385, 1, 1\n",
      "0.1149844378232956, 0.07634948939085007, 0.11566203087568283, 0.6930040121078491, 1, 3\n",
      "0.08562628924846649, 0.0446595661342144, 0.098932184278965, 0.770781934261322, 1, 3\n",
      "0.023307431489229202, 0.012882755137979984, 0.02557847648859024, 0.9382312893867493, 1, 3\n",
      "0.3109447658061981, 0.07310651242733002, 0.1605275571346283, 0.4554211497306824, 1, 3\n",
      "0.04798244312405586, 0.9191737771034241, 0.01583028957247734, 0.017013484612107277, 1, 1\n",
      "0.018627816811203957, 0.961225688457489, 0.00674019567668438, 0.013406328856945038, 1, 1\n",
      "0.050293855369091034, 0.017363574355840683, 0.12959811091423035, 0.802744448184967, 1, 3\n",
      "0.31738874316215515, 0.6108156442642212, 0.018949255347251892, 0.05284636467695236, 1, 1\n",
      "0.029685167595744133, 0.9559203386306763, 0.007001376245170832, 0.0073930975049734116, 1, 1\n",
      "0.2014112025499344, 0.6664064526557922, 0.03815942630171776, 0.09402287751436234, 1, 1\n",
      "0.21360880136489868, 0.13375599682331085, 0.06144854053854942, 0.5911867022514343, 1, 3\n",
      "0.6921499371528625, 0.0814405083656311, 0.04891538247466087, 0.17749415338039398, 1, 0\n",
      "0.02409350872039795, 0.9379709362983704, 0.01793413609266281, 0.02000143937766552, 1, 1\n",
      "0.017625531181693077, 0.0036765416152775288, 0.014767616987228394, 0.9639302492141724, 1, 3\n",
      "0.3801251947879791, 0.010022247210144997, 0.09461881220340729, 0.5152337551116943, 1, 3\n",
      "0.2381519079208374, 0.5307633876800537, 0.024159666150808334, 0.20692500472068787, 1, 1\n",
      "0.01661768928170204, 0.959924578666687, 0.008752948604524136, 0.014704901725053787, 1, 1\n",
      "0.020381223410367966, 0.9244834780693054, 0.014549355022609234, 0.04058597609400749, 1, 1\n",
      "0.5209692120552063, 0.06958051025867462, 0.2104446440935135, 0.19900567829608917, 1, 0\n",
      "0.0911024957895279, 0.0965804010629654, 0.048872970044612885, 0.7634441256523132, 1, 3\n",
      "0.32004183530807495, 0.012817966751754284, 0.07038137316703796, 0.5967589020729065, 1, 3\n",
      "0.7716984748840332, 0.04200908914208412, 0.11342465132474899, 0.07286777347326279, 1, 0\n",
      "0.40719905495643616, 0.06880053132772446, 0.08151508122682571, 0.4424853026866913, 1, 3\n",
      "0.21283237636089325, 0.6985388994216919, 0.033903513103723526, 0.054725196212530136, 1, 1\n",
      "0.017034798860549927, 0.9603890180587769, 0.00923824217170477, 0.013338048942387104, 1, 1\n",
      "0.19863082468509674, 0.030403848737478256, 0.28775838017463684, 0.48320695757865906, 1, 3\n",
      "0.5432218909263611, 0.019223500043153763, 0.08510217070579529, 0.35245245695114136, 1, 0\n",
      "0.015806715935468674, 0.0033569203224033117, 0.113059401512146, 0.8677769303321838, 1, 3\n",
      "0.14980562031269073, 0.07332535833120346, 0.030379323288798332, 0.7464897036552429, 1, 3\n",
      "0.2577895224094391, 0.09346219897270203, 0.12354027479887009, 0.5252079963684082, 1, 3\n",
      "0.24654603004455566, 0.1376638561487198, 0.08799110352993011, 0.5277990102767944, 1, 3\n",
      "0.019191399216651917, 0.0050327288918197155, 0.4943438172340393, 0.4814320504665375, 2, 2\n",
      "0.6006799340248108, 0.028624868020415306, 0.33925288915634155, 0.03144228085875511, 2, 0\n",
      "0.013574369251728058, 0.002076009986922145, 0.6460165977478027, 0.33833298087120056, 2, 2\n",
      "0.04584696143865585, 0.004270882345736027, 0.9080274105072021, 0.04185471311211586, 2, 2\n",
      "0.05218629539012909, 0.004546960350126028, 0.7962140440940857, 0.14705264568328857, 2, 2\n",
      "0.013676959089934826, 0.008962719701230526, 0.6104477047920227, 0.3669126033782959, 2, 2\n",
      "0.05531467869877815, 0.008069719187915325, 0.8866556286811829, 0.049959946423769, 2, 2\n",
      "0.031957268714904785, 0.005006571300327778, 0.9019567370414734, 0.061079416424036026, 2, 2\n",
      "0.027706686407327652, 0.007984928786754608, 0.6003834009170532, 0.3639250099658966, 2, 2\n",
      "0.10532491654157639, 0.023920856416225433, 0.7988723516464233, 0.07188183814287186, 2, 2\n",
      "0.011912882328033447, 0.007562931161373854, 0.9340440034866333, 0.0464802086353302, 2, 2\n",
      "0.1626337468624115, 0.006323745474219322, 0.4361426830291748, 0.39489978551864624, 2, 2\n",
      "0.09034828841686249, 0.014481494203209877, 0.6211300492286682, 0.2740401327610016, 2, 2\n",
      "0.06108928471803665, 0.008207645267248154, 0.8949808478355408, 0.03572214022278786, 2, 2\n",
      "0.04610436037182808, 0.004467019345611334, 0.8994751572608948, 0.04995343089103699, 2, 2\n",
      "0.15276704728603363, 0.014938422478735447, 0.6809471845626831, 0.1513472944498062, 2, 2\n",
      "0.22267593443393707, 0.00635879673063755, 0.568886399269104, 0.20207884907722473, 2, 2\n",
      "0.2009250372648239, 0.04554659128189087, 0.5181121230125427, 0.23541629314422607, 2, 2\n",
      "0.03222224488854408, 0.0026390240527689457, 0.8606347441673279, 0.10450401902198792, 2, 2\n",
      "0.023152198642492294, 0.003031585831195116, 0.9036595225334167, 0.07015671581029892, 2, 2\n",
      "0.011066715233027935, 0.007902668789029121, 0.9357916116714478, 0.04523902013897896, 2, 2\n",
      "0.017313022166490555, 0.0026977858506143093, 0.337882936000824, 0.642106294631958, 2, 3\n",
      "0.10712805390357971, 0.013047097250819206, 0.7990868091583252, 0.08073798567056656, 2, 2\n",
      "0.07263264805078506, 0.006672170013189316, 0.5320780873298645, 0.38861706852912903, 2, 2\n",
      "0.061783358454704285, 0.01057316642254591, 0.5596678256988525, 0.3679756224155426, 2, 2\n",
      "0.04169706627726555, 0.00565288495272398, 0.24640682339668274, 0.7062431573867798, 2, 3\n",
      "0.01186993159353733, 0.002135211369022727, 0.8770470023155212, 0.10894792526960373, 2, 2\n",
      "0.18690402805805206, 0.01786941848695278, 0.5761837363243103, 0.21904286742210388, 2, 2\n",
      "0.02322850003838539, 0.004171190783381462, 0.766986608505249, 0.20561368763446808, 2, 2\n",
      "0.09194385260343552, 0.003022134304046631, 0.34545817971229553, 0.5595758557319641, 2, 3\n",
      "0.34812718629837036, 0.03746118023991585, 0.5426068902015686, 0.07180469483137131, 2, 2\n",
      "0.022275641560554504, 0.003913855645805597, 0.945745587348938, 0.028065012767910957, 2, 2\n",
      "0.24056948721408844, 0.010758205316960812, 0.5218815803527832, 0.22679071128368378, 2, 2\n",
      "0.015028162859380245, 0.01845517009496689, 0.30174797773361206, 0.664768636226654, 2, 3\n",
      "0.05733633413910866, 0.3108155131340027, 0.5418480634689331, 0.09000011533498764, 2, 2\n",
      "0.050032537430524826, 0.002759197959676385, 0.9120137095451355, 0.03519462049007416, 2, 2\n",
      "0.04168979823589325, 0.007350396364927292, 0.9221612811088562, 0.028798488900065422, 2, 2\n",
      "0.03223619982600212, 0.004004047252237797, 0.6529907584190369, 0.31076905131340027, 2, 2\n",
      "0.22961540520191193, 0.01311054453253746, 0.6470661759376526, 0.11020784080028534, 2, 2\n",
      "0.08101674914360046, 0.0037038051523268223, 0.8611769676208496, 0.05410246551036835, 2, 2\n",
      "0.0975932627916336, 0.004221974406391382, 0.8467249870300293, 0.05145976319909096, 2, 2\n",
      "0.10804834961891174, 0.01517906691879034, 0.5673586130142212, 0.30941399931907654, 2, 2\n",
      "0.02502284198999405, 0.004843768198043108, 0.8591071963310242, 0.11102622002363205, 2, 2\n",
      "0.0448346845805645, 0.016437845304608345, 0.8361762762069702, 0.10255125910043716, 2, 2\n",
      "0.05305773392319679, 0.008007380180060863, 0.8638241291046143, 0.07511082291603088, 2, 2\n",
      "0.10561012476682663, 0.07542325556278229, 0.6249203681945801, 0.1940462440252304, 2, 2\n",
      "0.09148920327425003, 0.03926847130060196, 0.21816504001617432, 0.6510772109031677, 2, 3\n",
      "0.11217182874679565, 0.026372957974672318, 0.7673220038414001, 0.09413323551416397, 2, 2\n",
      "0.43167969584465027, 0.0242439154535532, 0.37937086820602417, 0.1647055298089981, 2, 0\n",
      "0.08903294056653976, 0.008232765831053257, 0.7756925821304321, 0.1270417422056198, 2, 2\n",
      "0.00838764850050211, 0.0028788296040147543, 0.06130576506257057, 0.9274277091026306, 3, 3\n",
      "0.07604457437992096, 0.00443692272529006, 0.11590790748596191, 0.8036106824874878, 3, 3\n",
      "0.03204847499728203, 0.0031808307394385338, 0.06855916231870651, 0.8962115049362183, 3, 3\n",
      "0.26437973976135254, 0.14467744529247284, 0.037478018552064896, 0.5534647703170776, 3, 3\n",
      "0.03111342526972294, 0.007182259578257799, 0.02896268665790558, 0.932741641998291, 3, 3\n",
      "0.009336436167359352, 0.004667503293603659, 0.11483533680438995, 0.8711608052253723, 3, 3\n",
      "0.009167875163257122, 0.015608745627105236, 0.09472700953483582, 0.8804963231086731, 3, 3\n",
      "0.020099356770515442, 0.0029336262959986925, 0.07083185017108917, 0.9061352014541626, 3, 3\n",
      "0.03663710504770279, 0.004024918656796217, 0.14059244096279144, 0.8187455534934998, 3, 3\n",
      "0.008875406347215176, 0.0029081623069941998, 0.15188956260681152, 0.8363268375396729, 3, 3\n",
      "0.01433512382209301, 0.06140398234128952, 0.09694623947143555, 0.8273146152496338, 3, 3\n",
      "0.02181076630949974, 0.01326837670058012, 0.07281918078660965, 0.8921017050743103, 3, 3\n",
      "0.008370060473680496, 0.005446107592433691, 0.01935148611664772, 0.9668322801589966, 3, 3\n",
      "0.1006929874420166, 0.008671835996210575, 0.08608102798461914, 0.8045541048049927, 3, 3\n",
      "0.019014958292245865, 0.00884342286735773, 0.09365769475698471, 0.8784838318824768, 3, 3\n",
      "0.044534146785736084, 0.039003223180770874, 0.07134327292442322, 0.8451193571090698, 3, 3\n",
      "0.08747351169586182, 0.010418808087706566, 0.12207116931676865, 0.7800365090370178, 3, 3\n",
      "0.33354613184928894, 0.027533484622836113, 0.19286690652370453, 0.44605356454849243, 3, 3\n",
      "0.021569697186350822, 0.007961658760905266, 0.09051106870174408, 0.8799576163291931, 3, 3\n",
      "0.029830852523446083, 0.0069397445768117905, 0.3295743465423584, 0.6336550712585449, 3, 3\n",
      "0.03571755439043045, 0.00918892677873373, 0.3747708797454834, 0.5803226232528687, 3, 3\n",
      "0.08675984293222427, 0.00886857882142067, 0.09900247305631638, 0.8053691387176514, 3, 3\n",
      "0.06345746666193008, 0.008260567672550678, 0.030027853325009346, 0.8982540965080261, 3, 3\n",
      "0.045669712126255035, 0.004429417196661234, 0.022910142317414284, 0.9269907474517822, 3, 3\n",
      "0.12978391349315643, 0.027693476527929306, 0.1264568269252777, 0.7160658240318298, 3, 3\n",
      "0.17369960248470306, 0.03187232092022896, 0.4421946704387665, 0.3522334694862366, 3, 2\n",
      "0.009070189669728279, 0.0024886969476938248, 0.09947264939546585, 0.8889684677124023, 3, 3\n",
      "0.0276145339012146, 0.004521127790212631, 0.08869138360023499, 0.8791729807853699, 3, 3\n",
      "0.01920153573155403, 0.0023578142281621695, 0.11538370698690414, 0.8630569577217102, 3, 3\n",
      "0.006921989843249321, 0.0020353756844997406, 0.022264499217271805, 0.968778133392334, 3, 3\n",
      "0.04700753465294838, 0.001860033138655126, 0.14835527539253235, 0.8027772307395935, 3, 3\n",
      "0.01415772270411253, 0.015190832316875458, 0.10799556970596313, 0.8626558780670166, 3, 3\n",
      "0.025210266932845116, 0.04040722921490669, 0.05585343763232231, 0.8785291314125061, 3, 3\n",
      "0.016848910599946976, 0.010647747665643692, 0.1478976607322693, 0.8246057033538818, 3, 3\n",
      "0.07650155574083328, 0.007998732849955559, 0.07180197536945343, 0.843697726726532, 3, 3\n",
      "0.014341103844344616, 0.011028541252017021, 0.07587563991546631, 0.8987547159194946, 3, 3\n",
      "0.015667421743273735, 0.00853334553539753, 0.03393082693219185, 0.9418684244155884, 3, 3\n",
      "0.035535719245672226, 0.007766272407025099, 0.2176084816455841, 0.7390894889831543, 3, 3\n",
      "0.08868618309497833, 0.0052897692658007145, 0.25311484932899475, 0.6529092192649841, 3, 3\n",
      "0.1803114265203476, 0.01991298794746399, 0.20171219110488892, 0.5980634093284607, 3, 3\n",
      "0.03339463844895363, 0.0022364447358995676, 0.20001919567584991, 0.7643496990203857, 3, 3\n",
      "0.03756295517086983, 0.01912389136850834, 0.052537500858306885, 0.8907756209373474, 3, 3\n",
      "0.017373232170939445, 0.003098784014582634, 0.019961608573794365, 0.9595663547515869, 3, 3\n",
      "0.05652894079685211, 0.006726379971951246, 0.08513092249631882, 0.8516137003898621, 3, 3\n",
      "0.012411326169967651, 0.01680622808635235, 0.059229955077171326, 0.9115524888038635, 3, 3\n",
      "0.016261162236332893, 0.0017955454532057047, 0.07263178378343582, 0.9093115329742432, 3, 3\n",
      "0.06686485558748245, 0.002722556935623288, 0.15270031988620758, 0.7777122855186462, 3, 3\n",
      "0.044776033610105515, 0.012906979769468307, 0.07438974827528, 0.8679272532463074, 3, 3\n",
      "0.023677697405219078, 0.00619274377822876, 0.059537287801504135, 0.910592257976532, 3, 3\n",
      "0.01210083533078432, 0.00648947898298502, 0.21028634905815125, 0.7711234092712402, 3, 3\n"
     ]
    }
   ],
   "source": [
    "print(\"GNB, GNC, GPB, GPC, label, pred\")\n",
    "for prob, lable, pred in zip(logits, all_labels, all_preds):\n",
    "    print(f'{prob[0]}, {prob[1]}, {prob[2]}, {prob[3]}, {lable}, {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GNB': 0, 'GNC': 1, 'GPB': 2, 'GPC': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
